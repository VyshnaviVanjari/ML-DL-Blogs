{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hard Drive Failure Prediction\n",
    ">Machine Learning for Hard Drive Failure Prediction\n",
    "\n",
    "- toc: true\n",
    "- badges: true\n",
    "- comments: true\n",
    "- image: images/hdd.png\n",
    "- author: Vyshnavi Vanjari\n",
    "- categories: [Machine Learning]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "    <img src=\"https://user-images.githubusercontent.com/54760460/94100169-8162eb00-fe4a-11ea-93eb-b5972127ca1d.png\" alt=\"Hard Drive Failure\"/><br />\n",
    "    <center><a href=\"https://vsbytes.com/\">Image Source</a></center>\n",
    "</p>\n",
    "\n",
    "\n",
    "## Introduction to the Business Problem\n",
    "\n",
    "- Hard drives are essential parts of data storage. When a hard disk drive malfunctions and data can’t be accessed, it is called a Hard Disk Drive Failure.\n",
    "- There are many reasons for the failure of a Hard Drive like high magnetic fields, exposure to heat, or any normal operation, data corruption, human error, power issues etc,.\n",
    "- Failure of a hard drive can be immediate, progressive or limited. Data may be totally destroyed or partially destroyed or can be totally recovered.\n",
    "- So predicting the failures can be helpful in data backup prior to failure and we can replace the drive with a good one without any loss of our data.\n",
    "- A Hard drive failure prediction method called SMART(Self-Monitoring, Analysis and Reporting Technology) has been proposed to constantly monitor the drives to predict failures in order to reduce the risk of data loss.\n",
    "- SMART attributes represent Hard Drive health statistics such as the number of scan errors, reallocation counts and probational counts of a Hard Drive.\n",
    "- In this case study, we are using Backblaze Hard Drive Stats Q3 2019 to predict failures of Hard Drives.\n",
    "- Backblaze provides the data containing information from different manufacturers and different models with all the SMART parameters.\n",
    "- Using the data provided by Backblaze, we apply different machine learning algorithms to predict the failures.\n",
    "\n",
    "To pose the problem in a better way, we can say that we need to predict if a hard drive is going to fail in the next 'N' days('N' is optimal value). If we can predict failure before 'N' days, we get sufficient time to retrieve the data and can replace that with a new drive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extraction\n",
    "\n",
    "The Backblaze data center takes a snapshot of each operational hard drive everyday. The snapshot includes basic drive information along with the SMART statistics reported by the drive. All drives' snapshots for a given day are collected into a file consisting of a row for each active hard drive. The format of this file is 'csv'(Comma Separated Values). Each day this file is named in the format YYYY-MM-DD.csv, for example, 2019-08-01.csv.\n",
    "\n",
    "The columns of the file are as follows:\n",
    "\n",
    "- Date – The date of the file in yyyy-mm-dd format. \n",
    "- Serial Number – The manufacturer-assigned serial number of the drive. \n",
    "- Model – The manufacturer-assigned model number of the drive. \n",
    "- Capacity – The drive capacity in bytes. \n",
    "- Failure – Contains a '0' if the drive is OK. Contains a '1' if this is the last day the drive was operational before failing.\n",
    "- SMART Parameters\n",
    "\n",
    "We can download data from Backblaze website:\n",
    "[https://www.backblaze.com/b2/hard-drive-test-data.html#downloading-the-raw-hard-drive-test-data](https://www.backblaze.com/b2/hard-drive-test-data.html#downloading-the-raw-hard-drive-test-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Existing Approaches\n",
    "\n",
    "Research Paper: [https://hal.archives-ouvertes.fr/hal-01703140/document](https://hal.archives-ouvertes.fr/hal-01703140/document)\n",
    "\n",
    "- In this research paper, machine learning algorithms were applied on the 2013 Backblaze dataset and all the models’ performances were compared. \n",
    "- 12 million samples from 47,793 drives including 31 models from 5 manufacturers were used. Of the 12 million samples, only 2586 samples have failure labels set to 1 and others are healthy samples which makes the dataset highly imbalanced. \n",
    "- Defined a time window for failure which predicts whether the hard drive is going to fail in the next N days. \n",
    "- Some SMART parameters were pre selected based on high correlation to failure events. SMART parameters - 5, 12, 187, 188, 189, 190, 198, 199, 200. \n",
    "- To handle data imbalance, SMOTE technique was applied. \n",
    "- Some failure samples share the exact same feature values as healthy samples leading to the impossibility to discriminate against them and so certain categories of failure samples were filtered.\n",
    "- Logistic Regression, Support Vector Machine, Random Forest Classifier, Gradient Boosting Decision Trees - All these machine\n",
    "learning algorithms were applied on the final data. \n",
    "- It was observed that RF and GBDT resulted in good precision and recall. \n",
    "- RF provided best performances with 95% precision and 67% recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvements to the Exisiting Approaches\n",
    "\n",
    "- For every hard drive, SMART parameters are recorded everyday and these parameters are very important in predicting failures.\n",
    "- We can extract time series features from these SMART parameters like rolling window features(mean, std), expanding window features(mean, std), exponential smoothing, lags, etc,.\n",
    "- In the research paper, it was given that SMOTE technique used for data balancing didn't contributed much in predicting failures. Hence upsampling is used to balance the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "- False alarm rate is a good choice for balanced datasets but, as operational datasets are extremely unbalanced in favour of working drives, even a low false alarm rate in the range of 1% could translate into poor performances. Therefore, we report precision, recall metrics and f1-score.\n",
    "\n",
    "- False Alarm Ratio = False Alarms/Total number of alarms<br />\n",
    "i.e., False Alarm Ratio = (Number of drives wrongly detected as failures)/(Total number of actually failed drives)\n",
    "\n",
    "- Precision = true positive/(true positive + false positive)<br />\n",
    "i.e., Precision = (Number of drives that are actually failures)/(Total number of drives that are predicted to be failures)\n",
    "\n",
    "- Recall = true positive/(true positive + false negative)<br />\n",
    "i.e., Recall = (Number of drives predicted correctly as failures)/(Total number of drives that are actually failures)\n",
    "\n",
    "- F1-Score = 2 * (Precision * Recall) / (Precision + Recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "Backblaze uses the following five SMART stats as a means to help determine if a drive is going to fail.\n",
    "\n",
    "    ATTRIBUTE       DESCRIPTION\n",
    "    SMART 5         Reallocated Sectors Count\n",
    "    SMART 187       Reported Uncorrectable Errors\n",
    "    SMART 188       Command Timeout\n",
    "    SMART 197       Current Pending Sector Count\n",
    "    SMART 198       Uncorrectable Sector Count\n",
    "\n",
    "Along with above features, we also selected other SMART features - SMART 5, 9, 12, 187, 188, 189, 190, 193, 194, 197, 198, 199, 200, 241, 242."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"july_august\"+\"\\\\\"+listdir(\"july_august\")[0])\n",
    "for file in listdir(\"july_august\")[1:]:\n",
    "  df=df.append(pd.read_csv(\"july_august\"+\"\\\\\"+file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "![image](https://user-images.githubusercontent.com/54760460/94002334-64310c80-fdb7-11ea-9b9b-3b8b47492618.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe, there are normalized features for SMART parameters.<br />\n",
    "But, we only select raw values. Also, due to limited computational power, we select only segate models' July and August months' data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=['date','model','serial_number','capacity_bytes', 'failure','smart_5_raw','smart_9_raw','smart_12_raw','smart_187_raw',\n",
    "             'smart_188_raw','smart_189_raw','smart_190_raw','smart_193_raw','smart_194_raw','smart_197_raw','smart_198_raw',\n",
    "             'smart_199_raw','smart_200_raw','smart_241_raw','smart_242_raw']\n",
    "df_new=df[features]\n",
    "df_segate=df_new[df_new['model']=='ST4000DM000']\n",
    "for model in df_new['model'].unique()[1:]:\n",
    "    if model[:2]  == 'ST' or model[:2] == 'Se':\n",
    "        print(model)\n",
    "        row_df=df_new[df_new['model']==model]\n",
    "        df_segate=pd.concat([df_segate,row_df])\n",
    "df_segate.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Analysis: Failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.set_style('darkgrid')\n",
    "sb.distplot(df_segate['failure'])\n",
    "plt.title(\"Distribution Plot of failure\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/54760460/94002994-444e1880-fdb8-11ea-9c13-5952309bb68d.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_segate[df_segate['failure']==0].shape\n",
    "(5053997, 20)\n",
    "    \n",
    "df_segate[df_segate['failure']==1].shape\n",
    "(367, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From the above plot, we can observe that the data set is highly imbalanced with only a few number of failures(failure=1)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Analysis: smart_5_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.boxplot(x='failure',y='smart_5_raw',data=df_segate)\n",
    "plt.title(\"Box Plot to detect failure based on smart_5_raw\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/54760460/94003510-ee2da500-fdb8-11ea-8c8e-592397c561a1.png)\n",
    "\n",
    "**From the above plot, we can observe that most of the drives are working for different range values of smart_5_raw and most of failures occurred when smart_5_raw is below the range of 10000**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.distplot(df_segate[df_segate['failure']==1]['smart_5_raw'].dropna())\n",
    "plt.title(\"Distribution Plot of smart_5_raw for failed drives\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/54760460/94003648-27661500-fdb9-11ea-802a-d66ec4b02aa3.png)\n",
    "\n",
    "**From the above plots, we can observe that most of the failures(failure=1) occurred when smart_5_raw values are in the range of 0 to 10000 and few failures in the range of 20000 and one failure in the range of 70000**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Analysis: smart_9_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.boxplot(x='failure',y='smart_9_raw',data=df_segate)\n",
    "plt.title(\"Box Plot to detect failure based on smart_9_raw\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/54760460/94004052-b70bc380-fdb9-11ea-8d15-71dba6647e63.png)\n",
    "\n",
    "**From the above plot, we can observe that working drives have smart_9_raw values in the range of 12000 to 29000. However, some failed drives also have smart_9_raw values in the range of 10000 to 18000**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.distplot(df_segate[df_segate['failure']==1]['smart_9_raw'].dropna())\n",
    "plt.title(\"Distribution Plot of smart_9_raw for failed drives\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/54760460/94004148-de629080-fdb9-11ea-91ae-64c486340776.png)\n",
    "\n",
    "**From the above plots, we can observe that most of the failures occurred when smart_9_raw is in the range of 10000 to 20000**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Analysis: smart_187_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.boxplot(x='failure',y='smart_187_raw',data=df_segate)\n",
    "plt.title(\"Box Plot to detect failure based on smart_187_raw\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/54760460/94004304-1bc71e00-fdba-11ea-8387-1dea99988bc0.png)\n",
    "\n",
    "**From the above plot, we can observe that working drives have smart_187_raw values in different ranges and most of the failures when smart_187_raw value is 0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.distplot(df_segate[df_segate['failure']==1]['smart_187_raw'].dropna())\n",
    "plt.title(\"Distribution Plot of smart_187_raw for failed drives\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/54760460/94004378-3ac5b000-fdba-11ea-9a45-3a93447f05c5.png)\n",
    "\n",
    "**From the above plots, we can observe that most of the failures occurred when smart_187_raw values are in the range of 0 to 1000 and only one failure occurred when the values are greater than 50000**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Analysis: smart_188_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.boxplot(x='failure',y='smart_188_raw',data=df_segate)\n",
    "plt.title(\"Box Plot to detect failure based on smart_188_raw\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/54760460/94006322-4a92c380-fdbd-11ea-9812-dc098e882a95.png)\n",
    "\n",
    "**From the above plot, we can observe that working drives have smart_188_raw values in very high ranges of 1e11**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Analysis: smart_193_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.boxplot(x='failure',y='smart_193_raw',data=df_segate)\n",
    "plt.title(\"Box Plot to detect failure based on smart_193_raw\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/54760460/94007403-086a8180-fdbf-11ea-88b6-3bdf69391419.png)\n",
    "\n",
    "**From the above plot, we can observe that working drives have smart_193_raw values in different ranges from 0 to 1400000**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.distplot(df_segate[df_segate['failure']==1]['smart_193_raw'].dropna())\n",
    "plt.title(\"Distribution Plot of smart_193_raw for failed drives\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/54760460/94007467-21733280-fdbf-11ea-8ae6-326711e201cb.png)\n",
    "\n",
    "**From the above plots and analysis, we can observe that most of the failures occurred when smart_193_raw is in the range of 0 to 10000 and few failures in the range of 10000 to 50000**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Analysis: smart_194_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.boxplot(x='failure',y='smart_194_raw',data=df_segate)\n",
    "plt.title(\"Box Plot to detect failure based on smart_194_raw\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/54760460/94008381-909d5680-fdc0-11ea-88c4-a7aacac79dfa.png)\n",
    "\n",
    "**From the above plot, we can observe that both working and failed drives have smart_194_raw values in almost same ranges from 25 to 35**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.distplot(df_segate[df_segate['failure']==1]['smart_194_raw'].dropna())\n",
    "plt.title(\"Distribution Plot of smart_194_raw for failed drives\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/54760460/94008471-b4f93300-fdc0-11ea-9b76-56dfa2f0ace5.png)\n",
    "\n",
    "**From the above plots, we can observe that most failures occurred when smart_194_raw values are in the range of 20 to 40**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Analysis: smart_197_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.boxplot(x='failure',y='smart_197_raw',data=df_segate)\n",
    "plt.title(\"Box Plot to detect failure based on smart_197_raw\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/54760460/94008770-29cc6d00-fdc1-11ea-9359-0271ab8cfb9c.png)\n",
    "\n",
    "**From the above plot, we can observe that working drives have smart_197_raw values in the range of 0 to 3000 and also failed drives have same range of values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.distplot(df_segate[df_segate['failure']==1]['smart_197_raw'].dropna())\n",
    "plt.title(\"Distribution Plot of smart_197_raw for failed drives\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/54760460/94008922-5aaca200-fdc1-11ea-80fc-355a541b01cd.png)\n",
    "\n",
    "**From the above plots, we can observe that all failures occurred when smart_197_raw values are in the range of 0 to 3000 and most failures occurred when the value is 0**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Analysis: smart_198_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.boxplot(x='failure',y='smart_198_raw',data=df_segate)\n",
    "plt.title(\"Box Plot to detect failure based on smart_198_raw\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/54760460/94009045-83cd3280-fdc1-11ea-8d37-b8a6b504425b.png)\n",
    "\n",
    "**From the above plot, we can observe that working drives have smart_198_raw values in the range of 0 to 3000 and failed drives also have almost same range of values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.distplot(df_segate[df_segate['failure']==1]['smart_198_raw'].dropna())\n",
    "plt.title(\"Distribution Plot of smart_198_raw for failed drives\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/54760460/94009177-bd05a280-fdc1-11ea-974f-692287c30bd8.png)\n",
    "\n",
    "**From above plot, we can observe that all the failures occurred when smart_198_raw values are in the range of 0 to 2500 and most failures occurred when the value is 0**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Analysis: smart_241_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.boxplot(x='failure',y='smart_241_raw',data=df_segate)\n",
    "plt.title(\"Box Plot to detect failure based on smart_241_raw\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/54760460/94009469-21286680-fdc2-11ea-930d-99bb6c0d8ccb.png)\n",
    "\n",
    "**From the above plot, we can observe that working drives have smart_241_raw values in different ranges and have high values in the range of 1e11**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.distplot(df_segate[df_segate['failure']==1]['smart_241_raw'].dropna())\n",
    "plt.title(\"Distribution Plot of smart_241_raw for failed drives\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/54760460/94009668-6b114c80-fdc2-11ea-834d-d3ac1855b57d.png)\n",
    "\n",
    "**From the above plot, we can observe that failures occurred when smart_241_raw values are in very high ranges of 0.5e11 to 0.6e11**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Analysis: smart_242_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.boxplot(x='failure',y='smart_242_raw',data=df_segate)\n",
    "plt.title(\"Box Plot to detect failure based on smart_242_raw\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/54760460/94010455-96486b80-fdc3-11ea-9861-2230de7e32f1.png)\n",
    "\n",
    "**From the above plot, we can observe that working drives have smart_242_raw values in different ranges of values from 0 to 2.5e13**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.distplot(df_segate[df_segate['failure']==1]['smart_242_raw'].dropna())\n",
    "plt.title(\"Distribution Plot of smart_242_raw for failed drives\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/54760460/94010707-ed4e4080-fdc3-11ea-90c9-3c2ca265983b.png)\n",
    "\n",
    "**From the above plot and analysis, we can observe that most of the failures occurred when smart_242_raw values are in the range of 1.062508e+10 to 1.5e+11**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Univariate Analyses:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From all the above plots, we can observe that the failures occurred for different ranges of smart attributes' values.<br />\n",
    "Most failures occurred when smart attributes' values are 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values\n",
    "\n",
    "We remove features which have missing values' percentage greater than 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''percentage of mssing values in each column'''\n",
    "features_new_2=[]\n",
    "for i in df_segate.columns:\n",
    "    print(i,null_sum[i]*100/df_segate.shape[0])\n",
    "    if null_sum[i]*100/df_segate.shape[0] > 30:\n",
    "        continue\n",
    "    features_new_2.append(i)\n",
    "df_segate=df_segate[features_new_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More than 25% of the values are missing in smart_189_raw, smart_200_raw. So dropping those columns**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filling Missing Values with mean\n",
    "\n",
    "We fill missing values in each column with their mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_segate['smart_5_raw']=df_segate['smart_5_raw'].fillna(df_segate['smart_5_raw'].mean())\n",
    "df_segate['smart_9_raw']=df_segate['smart_9_raw'].fillna(df_segate['smart_9_raw'].mean())\n",
    "df_segate['smart_12_raw']=df_segate['smart_12_raw'].fillna(df_segate['smart_12_raw'].mean())\n",
    "df_segate['smart_187_raw']=df_segate['smart_187_raw'].fillna(df_segate['smart_187_raw'].mean())\n",
    "df_segate['smart_188_raw']=df_segate['smart_188_raw'].fillna(df_segate['smart_188_raw'].mean())\n",
    "df_segate['smart_190_raw']=df_segate['smart_190_raw'].fillna(df_segate['smart_190_raw'].mean())\n",
    "df_segate['smart_193_raw']=df_segate['smart_193_raw'].fillna(df_segate['smart_193_raw'].mean())\n",
    "df_segate['smart_194_raw']=df_segate['smart_194_raw'].fillna(df_segate['smart_194_raw'].mean())\n",
    "df_segate['smart_197_raw']=df_segate['smart_197_raw'].fillna(df_segate['smart_197_raw'].mean())\n",
    "df_segate['smart_198_raw']=df_segate['smart_198_raw'].fillna(df_segate['smart_198_raw'].mean())\n",
    "df_segate['smart_199_raw']=df_segate['smart_199_raw'].fillna(df_segate['smart_199_raw'].mean())\n",
    "df_segate['smart_241_raw']=df_segate['smart_241_raw'].fillna(df_segate['smart_241_raw'].mean())\n",
    "df_segate['smart_242_raw']=df_segate['smart_242_raw'].fillna(df_segate['smart_242_raw'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing mean, std, min, max for smart_parameters row-wise\n",
    "\n",
    "We calculate Mean, Standard Deviation, Min, Max for SMART parameters row-wise and add them as new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_segate['mean']=df_segate[df_segate.columns[5:18]].mean(axis=1)\n",
    "df_segate['std']=df_segate[df_segate.columns[5:18]].std(axis=1)\n",
    "df_segate['min']=df_segate[df_segate.columns[5:18]].min(axis=1)\n",
    "df_segate['max']=df_segate[df_segate.columns[5:18]].max(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing negative capacity byte values\n",
    "\n",
    "We have observed that there are some negative values for capacity byte which are wrongly recorded. Hence dropping those rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=df_segate[df_segate['capacity_bytes']<10.0]\n",
    "ind=temp.index\n",
    "df_segate.drop(df_segate.index[list(ind)],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Sort the DataFrame by serial_number and date to extract time series features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new=pd.read_csv('df_segate_july_august_no_backtrack.csv')\n",
    "df_new_with_lag=df_new.sort_values(['serial_number','date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rolling Mean, Standard Deviation for SMART parameters - window 15\n",
    "\n",
    "We can write code using pd.DataFrame.shift function but we shouldn't do that directly for a column.<br /> \n",
    "We have different models with different serial numbers.<br /> \n",
    "For every unique serial number, we need to calculate rolling mean and standard deviation.<br />\n",
    "We can write code with shift function by looping over all the unique serial numbers but it takes a lot of time as there are many thousands of unique serial numbers.<br />\n",
    "Hence written code as pointed below.<br />\n",
    "Here we take window=15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serial_numbers=df_new_with_lag['serial_number'].values\n",
    "serial_number=df_new_with_lag['serial_number'].values[0]\n",
    "for column in tqdm(df_new_with_lag.columns[5:18]):\n",
    "    rolling_mean=[]\n",
    "    rolling_stdev=[]\n",
    "    for i in range(df_new_with_lag.shape[0]):\n",
    "        if serial_numbers[i]!=serial_numbers[i-1]:\n",
    "            values=[] \n",
    "            values.append(df_new_with_lag[column].values[i])\n",
    "            rolling_mean.append(mean(values))\n",
    "            rolling_stdev.append(values[-1])\n",
    "        else:\n",
    "            if(len(values)<15): \n",
    "                values.append(df_new_with_lag[column].values[i])\n",
    "                mean_=mean(values[0:len(values)])\n",
    "                stdev_=stdev(values[0:len(values)])\n",
    "                rolling_mean.append(mean_)\n",
    "                rolling_stdev.append(stdev_)\n",
    "            else:\n",
    "                values.append(df_new_with_lag[column].values[i])\n",
    "                mean_=mean(values[len(values)-15:len(values)])\n",
    "                stdev_=stdev(values[len(values)-15:len(values)])\n",
    "                rolling_mean.append(mean_)\n",
    "                rolling_stdev.append(stdev_)\n",
    "    df_new_with_lag[column+'_rolling_mean'] = rolling_mean\n",
    "    df_new_with_lag[column+'_rolling_stdev'] = rolling_stdev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expanding Mean for SMART parameters\n",
    "\n",
    "Below is the code snippet for extracting expanding mean features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serial_numbers=df_new_with_lag['serial_number'].values\n",
    "serial_number=df_new_with_lag['serial_number'].values[0]\n",
    "for column in tqdm(df_new_with_lag.columns[5:18]):\n",
    "    expanding_mean=[]\n",
    "    expanding_stdev=[]\n",
    "    for i in range(df_new_with_lag.shape[0]):\n",
    "        if serial_numbers[i]!=serial_numbers[i-1]:\n",
    "            values=[] \n",
    "            values.append(df_new_with_lag[column].values[i])\n",
    "            expanding_mean.append(sum(values))\n",
    "            expanding_stdev.append(values[-1])\n",
    "        else:\n",
    "            values.append(df_new_with_lag[column].values[i])\n",
    "            expanding_mean.append(mean(values))\n",
    "            expanding_stdev.append(stdev(values))\n",
    "    df_new_with_lag[column+'_expanding_mean'] = expanding_mean\n",
    "    df_new_with_lag[column+'_expanding_stdev'] = expanding_stdev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponential Smoothing\n",
    "\n",
    "For exponential smoothing, we have considered alpha=0.15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serial_numbers=df_new_with_lag['serial_number'].values\n",
    "serial_number=df_new_with_lag['serial_number'].values[0]\n",
    "alpha=0.15\n",
    "for column in tqdm(df_new_with_lag.columns[5:18]):\n",
    "    predicted_values=[]\n",
    "    for i in range(df_new_with_lag.shape[0]):\n",
    "        if serial_numbers[i]!=serial_numbers[i-1]:\n",
    "            predicted_value = (df_new_with_lag[column].values)[i]\n",
    "            predicted_values.append(predicted_value)\n",
    "        else:\n",
    "            predicted_value =(alpha*df_new_with_lag[column].values[i]) + ((1-alpha)*predicted_value)\n",
    "            predicted_values.append(predicted_value)\n",
    "    df_new_with_lag[column+'_exp_avg'] = predicted_values\n",
    "df_new_with_lag=df_new_with_lag.sort_values(['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change Failure status by backtracking\n",
    "\n",
    "We need to predict whether a drive is going to fail in the next 'N' days. Here we take N=15.<br />\n",
    "For this, we backtrack last 15 days' failures, i.e., if we have a failed drive, we mark failure as '1' for its previous 15 days. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backtrack last 15 days failures\n",
    "df_segate_backtrack=pd.read_csv(\"df_segate.csv\")\n",
    "new_date=[]\n",
    "for date in df_segate_backtrack['date']:\n",
    "    new_date.append(datetime.strptime(date,'%Y-%m-%d').date())\n",
    "df_segate_backtrack['date']=new_date\n",
    "\n",
    "failed=df_segate_backtrack[df_segate_backtrack['failure']==1]\n",
    "for serial_number in tqdm(failed.serial_number):\n",
    "    d=failed[failed['serial_number']==serial_number].date.values\n",
    "    temp=df_segate_backtrack[df_segate_backtrack['serial_number']==serial_number]\n",
    "    temp=temp[temp.date>=(d[0]-timedelta(days=15))]\n",
    "    temp=temp[temp.date<d[0]]\n",
    "    indices=temp.index\n",
    "    df_segate_backtrack.loc[indices,'failure']=1\n",
    "failed=df_segate_backtrack[df_segate_backtrack['failure']==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test split, Response Coding Categorical Features and Upsampling Minority Class\n",
    "\n",
    "After extracting the time series features and backtracking the failure status, we have splitted the data into train, cv, and test.<br />\n",
    "We have extracted features from Model ID and Serial Number and encoded them using response coding.<br />\n",
    "Below are the features extracted from Model ID and Serial Number:\n",
    " - model_char_count - eg: Model ID = 'ST12000NM0007', model_char_count = 13\n",
    " - model_second_last_char - eg: Model ID = 'ST12000NM0007', model_second_last_char = 'T7'\n",
    " - serial_number_second_last_char - eg: Serial Number = 'ZJV2SP8Y', serial_number_second_last_char = 'JY'\n",
    "\n",
    "By response coding model_second_last_char and serial_number_second_last_char, we get working and failing probabilites for these features out of which we used working probabilites as features.\n",
    "\n",
    "Finally, to balance the data, we upsampled the minority class(drives with failure=1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_train_upsample_df=resample(failed_train_df,replace=True,\n",
    "                                      n_samples=int(working_train_df.shape[0]),\n",
    "                                      random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "\n",
    "Data is standardized and used for Logistic Regression and Support Vector Machines.<br />\n",
    "For Naive Bayes, data is normalized.<br />\n",
    "Data is used directly for Random Forests and XGBoost as these are based on decision trees.<br />\n",
    "For all the below models, small code snippets and results are printed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "Logistic Regression is one of the useful algorithms for Binary Classification. The assumption that is made for this algorithm is that the data is linear.[[1]](https://hal.archives-ouvertes.fr/hal-01703140/document)[[2]](http://cs229.stanford.edu/proj2017/final-reports/5242080.pdf)<br />\n",
    "\n",
    "Hyper-parameter tuning is performed for different c_range values and L2 Regularization is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_range=[10**-4,10**-3,10**-2,10**-1,10**0,10**1,10**2,10**3,10**4]\n",
    "for c in c_range:\n",
    "    LR_model=LogisticRegression(C=c,penalty='l2')\n",
    "    LR_model.fit(train_standardized,train_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observed that f1 score is very less with logistic regression.<br />\n",
    "For c=0.01 and with penalty='l2', we got the best test f1 score.\n",
    "- train_f1_score=0.751285\n",
    "- cv_f1_score=0.009825\n",
    "- test_f1_score=0.009673"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine\n",
    "\n",
    "Support Vector Machine (SVM) alogirthm relies on finding the hyperplane that splits the two classes to predict while maximizing the distance with the closest data points.[[1]](https://hal.archives-ouvertes.fr/hal-01703140/document)[[2]](http://cs229.stanford.edu/proj2017/final-reports/5242080.pdf)<br />\n",
    "\n",
    "SGDClassifier with loss='Hinge' is used and hyper-parameter tuning is performed for different ranges of alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_range=[10**-4,10**-3,10**-2,10**-1,10**0,10**1,10**2,10**3,10**4]\n",
    "for a in alpha_range:\n",
    "    SGD_model=SGDClassifier(loss='hinge',alpha=a)\n",
    "    SGD_model.fit(train_standardized,train_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observed that test f1 score is less with SVM.<br />\n",
    "For alpha=0.0001, we got the best test f1 score.\n",
    "- train_f1_score=0.756617\n",
    "- cv_f1_score=0.011170\n",
    "- test_f1_score=0.01123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier\n",
    "\n",
    "Random forest is an ensemble technique based on Decision Tress. It takes a subset of observations and a subset of variables to build a group of decision trees. Predictions are made based on a vote among the different decision trees. Random forest model is chosen as it is robust to noise, caused by poorly correlated SMART features.[[1]](https://hal.archives-ouvertes.fr/hal-01703140/document)[[2]](http://cs229.stanford.edu/proj2017/final-reports/5242080.pdf)<br />\n",
    "\n",
    "Hyper-parameter tuning is performed for different values of n_estimators, max_depth, learning_rate.<br />\n",
    "Probabilities are calibrated using CalibratedClassiferCV before predicting the target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [100,150,200]\n",
    "max_depth = [7,  9]\n",
    "for i in n_estimators:\n",
    "    for j in max_depth:\n",
    "        rf_model = RandomForestClassifier(n_estimators=i, max_depth=j)\n",
    "        rf_model.fit(train_df_final_1,train_output)\n",
    "        cal_rf_model=CalibratedClassifierCV(rf_model,method='isotonic',cv='prefit')\n",
    "        cal_rf_model.fit(cv_df_final_1,cv_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observed that test f1 score is less with RandomForestClassifier.<br />\n",
    "For n_estiamtors=150, max_depth=9, we got the best test f1 score.\n",
    "- train_f1_score=0.27872\n",
    "- cv_f1_score=0.19057\n",
    "- test_f1_score=0.02651"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBClassifier\n",
    "\n",
    "Gradient boosted tree (GBT) is another ensemble technique based on decision trees. Training takes place in an iterative fashion with the goal of trying to minimize a loss function using a gradient descent method.[[1]](https://hal.archives-ouvertes.fr/hal-01703140/document)[[2]](http://cs229.stanford.edu/proj2017/final-reports/5242080.pdf)<br />\n",
    "\n",
    "Hyper-parameter tuning is performed for different values of n_estimators and max_depth.<br />\n",
    "Probabilities are calibrated using CalibratedClassiferCV before predicting the target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [100, 150, 200, 500, 1000]\n",
    "max_depth = [7, 9]\n",
    "for i in n_estimators:\n",
    "    for j in max_depth:\n",
    "        xgb_model = xgb.XGBClassifier(n_estimators=i, max_depth=j,tree_method='exact')\n",
    "        xgb_model.fit(train_df_final_1,train_output)\n",
    "        cal_xgb_model=CalibratedClassifierCV(xgb_model,method='isotonic',cv='prefit')\n",
    "        cal_xgb_model.fit(cv_df_final_1,cv_output)\n",
    "\n",
    "important_features=xgb_model.get_booster().get_score(importance_type=\"gain\")\n",
    "important_features_sorted=sorted(important_features.items(),key=lambda x:x[1], reverse=True)\n",
    "index_features=dict()\n",
    "i=0\n",
    "for column in train_df_final.columns:\n",
    "    index_features['f'+str(i)]=column \n",
    "    i+=1\n",
    "sorted_important_features_dict=dict()\n",
    "for i in important_features_sorted:\n",
    "    key=i[0]\n",
    "    sorted_important_features_dict[key]=index_features[key]\n",
    "print(\"Top 10 important features:\")\n",
    "list(sorted_important_features_dict.items())[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/54760460/94026129-eb41ad00-fdd6-11ea-9174-dd92c352bc9e.png)\n",
    "\n",
    "Observed that **XGBoost performed very well in predicting failed hard drives.**<br />\n",
    "Optimal hyper-parameters: **n_estimators=1000, max_depth=9**\n",
    "- **test f1_score: 0.929026**\n",
    "- **test Precison : 0.943334**\n",
    "- **test Recall : 0.915139**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "\n",
    "The Naive Bayes classifier makes the assumption that the value of a feature is conditionally independent of the value of another feature given some class label. Among the different techniques used for building Naive Bayes models, we chose Multinomial Naive Bayes, which assumes that the probability of a feature value given some class\n",
    "label is sampled from a multinomial distribution. For regularization, we use Laplace smoothing.[[2]](http://cs229.stanford.edu/proj2017/final-reports/5242080.pdf)<br />\n",
    "\n",
    "Hyper-parameter tuing is performed for different ranges of alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_range=[10**-4,10**-3,10**-2,10**-1,10**0,10**1,10**2,10**3,10**4]\n",
    "for a in alpha_range:\n",
    "    nb_clf=MultinomialNB(alpha=a, fit_prior=True)\n",
    "    nb_clf.fit(train_normalized,train_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observed that test f1 score is less with Naive Bayes.<br />\n",
    "For alpha=0.0001, we got the best test f1 score.\n",
    "- train_f1_score=0.625518\n",
    "- cv_f1_score=0.004463\n",
    "- test_f1_score=0.004925"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBClassifier with top 50 important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [100, 150, 200, 500, 1000]\n",
    "max_depth = [7, 9]\n",
    "for i in n_estimators:\n",
    "    for j in max_depth:\n",
    "        xgb_model_imp = xgb.XGBClassifier(n_estimators=i, max_depth=j,tree_method='exact')\n",
    "        xgb_model_imp.fit(train_df_final_imp_1,train_output)\n",
    "        cal_xgb_model_imp=CalibratedClassifierCV(xgb_model_imp,method='isotonic',cv='prefit')\n",
    "        cal_xgb_model_imp.fit(cv_df_final_imp_1,cv_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observed that f1 score is good with XGBClassifier when compared with all other models.<br />\n",
    "With n_estimators=1000, max_depth=9, we got the best f1 score.\n",
    "- **test f1_score : 0.935266**\n",
    "- **test Precison : 0.9370764762826719**\n",
    "- **test Recall : 0.9334619093539055**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble of RandomForestClassifier and XGBClassifier with top 50 important features\n",
    "\n",
    "Tried ensembling with different combinations of above classifiers with different weights using EnsembleVoteClassifier.<br />\n",
    "Ensemble of RandomForestClassifer and XGBClassifier with more weight to XGBClassifier resulted in good prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_vote_clf_imp = EnsembleVoteClassifier(clfs=[cal_rf_model_imp,cal_xgb_model_imp_new], voting='soft',refit=False,weights=[0.1,0.9])\n",
    "ensemble_vote_clf_imp.fit(train_df_final_imp_1,train_output)\n",
    "predicted_test_failure=ensemble_vote_clf_imp.predict(test_df_final_imp_1)\n",
    "test_f1_scores.append(f1_score(test_output, predicted_test_failure))\n",
    "print(\"test Precison :\",precision_score(test_output,predicted_test_failure))\n",
    "print(\"test Recall :\",recall_score(test_output,predicted_test_failure))\n",
    "print(\"test_f1_score=\",test_f1_scores[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- test Precison : 0.9477317554240631\n",
    "- test Recall : 0.926711668273867\n",
    "- test_f1_score= 0.9371038517796197"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparision of Different Models\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/54760460/94027919-feee1300-fdd8-11ea-84d8-4223c5c5e127.png)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/54760460/94028017-1b8a4b00-fdd9-11ea-842a-8f9906409542.png)\n",
    "\n",
    "By comparing results of above two tables, we can observe that with top 50 important features we are able to get good scores.<br /> \n",
    "We got best f1 score with XGBClassifier.<br />\n",
    "The next good model is RandomForest but it doesn't perform that well.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/54760460/94028555-8dfb2b00-fdd9-11ea-9081-34a91ec24801.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "\n",
    "1. From the above tables, we can observe that precision is good with random forests and xgboost but is very less with other classifiers.\n",
    "2. Recall is good with all classifiers except random forests.\n",
    "3. XGBClassifier predicted failed hard drives very well.\n",
    "4. Precision and recall scores are highest with XGBClassifier and also with ensemble of RF and XGB.\n",
    "\n",
    "5. **XGB With top 50 important features:**\n",
    "\n",
    "    Test Precison : 0.937076<br />\n",
    "    **Test Recall : 0.933461 -- Best**<br />\n",
    "    Test f1_score: 0.935266\n",
    "\n",
    "6. **Ensemble of XGB and RF With top 50 important features:**\n",
    "\n",
    "    **Test Precison : 0.947731 -- Best**<br />\n",
    "    Test Recall : 0.926711<br />\n",
    "    **Test f1_score: 0.937103 -- Best**\n",
    "\n",
    "7. We can observe that recall score is high with XGB classifier when top 50 features are used. Whereas f1-score and precision are high with ensemble of XGB and RF with top 50 features. We can choose any of these two models. Both the models are good.\n",
    "8. Extracted many time series features from given data like exponential averages, rolling mean, rolling standard deviation, expanding mean, expanding standard deviation, backtracked last 15 days' failures etc,.\n",
    "9. Top 10 important features for XGBClassifier are:\n",
    "    smart_188_raw_exp_avg\n",
    "    smart_5_raw\n",
    "    smart_197_raw\n",
    "    model_second_last_char_working\n",
    "    capacity_bytes\n",
    "    smart_199_raw_expanding_stdev\n",
    "    serial_second_last_char_working\n",
    "    smart_188_raw_expanding_mean\n",
    "    smart_9_raw_rolling_mean\n",
    "    smart_12_raw_rolling_stdev\n",
    "10. The above results are on SEGATE model hard drives' July and August months data. We can try with XGBoost modelling for other hard drives also.\n",
    "11. Recall is the important metric here. Our main aim to detect failed hard drives. In this case study, we have predicted hard drives that are going to fail in the next 15 days. If we can predict the drives that are going to fail few days before the failure, we can have sufficient time to retrieve data and replace them with new hard drives. It is somewhat fine if a drive predicted to be a failure is actually a working one. But the important aim here is recall: drives which are actually failures should be predicted as failures else if wrongly predicted as working ones, it may fail in future and data can't be retrieved.\n",
    "11. We got best recall score with XGBClassifier with top 50 important features: **0.933461**\n",
    "12. Limited data to arorund 5 million due to limited system capacity. Train data is around 3 millions(after upsampling around 6 million) With more amounts of data and feature engineering, we can further improve recall and f1 scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work\n",
    "\n",
    "1. From the above summary, we can observe that many of the time series features are useful in predicting hard drive failures.\n",
    "2. So, we can extract and experiment with more time series features for much better results.<br /> \n",
    "    Eg: double/triple exponential smoothing, lag features, etc,.\n",
    "3. We can use more data from all quarters of an Year to improve the results.\n",
    "4. We can also experiment with deep learning techniques to predict the failures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. [https://hal.archives-ouvertes.fr/hal-01703140/document](https://hal.archives-ouvertes.fr/hal-01703140/document)\n",
    "2. [http://cs229.stanford.edu/proj2017/final-reports/5242080.pdf](http://cs229.stanford.edu/proj2017/final-reports/5242080.pdf)\n",
    "3. [https://www.kaggle.com/vishakg/predicting-hdd-failures-using-ml](https://www.kaggle.com/vishakg/predicting-hdd-failures-using-ml)\n",
    "4. [https://en.wikipedia.org/wiki/Hard_disk_drive_failure](https://en.wikipedia.org/wiki/Hard_disk_drive_failure)\n",
    "5. [https://www.backblaze.com/blog/backblaze-hard-drive-stats-q3-2019/](https://www.backblaze.com/blog/backblaze-hard-drive-stats-q3-2019/)\n",
    "6. [https://neurospace.io/blog/2018/10/predicting-hard-drive-failure-with-machine-learning/](https://neurospace.io/blog/2018/10/predicting-hard-drive-failure-with-machine-learning/)\n",
    "7. [https://vsbytes.com/hdd-vs-ssd/](https://vsbytes.com/hdd-vs-ssd/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thanks for Reading😃**\n",
    "\n",
    "**Complete code in github: [https://github.com/VyshnaviVanjari/HDDFailure](https://github.com/VyshnaviVanjari/HDDFailure)**\n",
    "\n",
    "**Reach me at Linkedin: [https://www.linkedin.com/in/vyshnavi-vanjari-57345896/](https://www.linkedin.com/in/vyshnavi-vanjari-57345896/)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
