{
  
    
        "post0": {
            "title": "Deep Text Corrector",
            "content": "Contents . Introduction . | Data Preparation . | Approaches to the Problem . | Evaluation Metrices . | Modelling . Encoder-Decoder Model with Teacher Forcing | Encoder-Decoder Model with Attention Mechanism | Transformers | Attention Mechanism with Transformer based tokenization | | Future Work . | References . | 1. Introduction . This project aims to correct grammatical mistakes in a given English text. | Grammatical errors are of different types, including errors in articles or determiners, prepositions, noun form, verb form, subject-verb agreement, pronouns, word choice, sentence structure, punctuation, capitalization, etc. | In this project, we aim to correct 16 types of grammatical errors. . Verb Tense | Subject-Verb agreement | Missing Verb | Article or Determiner | Noun Number | Noun Possessive | Pronoun Reference | Run-on Sentences, Comma Splices | Adjective/Adverb order | Linking words/Phrases | Poor Citation | Parallelism | Improper Preposition Forms | Word Contractions | Capitalization | Spelling | | . To achieve this, we train a Deep Learning Model which takes grammatically incorrect text as input and outputs grammatically corrected text. | Sequence-to-Sequence models are capable of correcting grammatical errors. | We try different models with different architectures starting from basic encoder-decoder models, encoder-decoder models with attention to Transformers. | . 2. Data Preparation . Downloaded an English Novel which has grammatically correct text.[1] | Did some basic pre-processing. Removed contents, chapters&#39; names, etc, to just have only paragraphs with text. | Did sentence detection using SPACY by setting custom boundaries.[2] | . Below is a small code snippet for setting custom boundaries. . Some of the sentences generated from above process have improper opening and closing quotes, braces. Corrected them to get linguistically meaningful units. | Removed sentences having only one word. | Once we got all the grammatically correct meaningful sentences, splitted the data into train, cv and test data. | Introduced 16 different types of grammatical errors into the datasets. | . Below is the code snippet for introducing 16 types of errors into the sentences. . After introducing errors into the sentences, introuduced space between words and punctuations in the sentences as punctuations need to be considered as seperate tokens while tokenization. Eg: &#39;I am good.&#39; - &#39;I am good .&#39; (space between good and fullstop) | With this, we get proper train, cv, test datas with grammatically incorrect input sentences and grammatically correct output sentences. | . Complete Code for Pre-Processing and Introducting errors @ https://github.com/VyshnaviVanjari/Deep_Text_Corrector/blob/main/preprocessing_add_errors.ipynb . 3. Approaches to the Problem . Encoder-Decoder models using RNNs is one of the approaches to this problem. However, this has a disadvatange of having a fixed length context vector which is incapable of remembering long sentences. [3] | The attention mechanism helps to memorize long source sentences in neural machine translation. Rather than building a single context vector from the encoder&#39;s last hidden state, attention creates shortcuts between the context vector and the entire source input.[4] | Attention mechanism is a very good approach to this problem but the only problem is with Out of Vocabulary(OOV) words. | Transformers are a great solution to this. Transformer tokenizers provide sub-word tokenization, byte-pair encoding, etc., and so there will be no problem in tokenizing OOV words.[5] | . 4. Evaluation Metrices . Chosen F0.5 as the evaluation metric similar to that in CoNLL-2014 shared task on grammatical error correction. F0.5 emphasizes precision twice as much as recall, while F1 weighs precision and recall equally. When a grammar checker is put into actual use, it is important that its proposed corrections are highly accurate in order to gain user acceptance. Neglecting to propose a correction is not as bad as proposing an erroneous correction.[6] [7] . Precision = True Positives/(True Positives + False Positives) . Recall = True Positives/(True Positives + False Negatives) . F1-Score = (2 Precision Recall) / (Precision + Recall) . Fbeta = ((1 + beta^2) Precision Recall) / (beta^2 * Precision + Recall) . F0.5-score (beta=0.5): More weight on precision, less weight on recall. . | . During inference, reported BLEU scores, perplexity along with F0.5 score.[8] [9] | . 5. Modelling . i. Encoder Decoder Model with Teacher Forcing . Below figure shows an encoder decoder model with teacher forcing. . Input: Harry say nothing. Expected Output: Harry said nothing. . | . . Data needs to be tokenized and embedded before sending to encoder and decoder. | As we can see in the above figure, we send the input sentence(embedded one) to the encoder. Here encoder and decoders are RNNs like LSTMs. | Encoder encodes the sentence and the last hidden state has all the sentence information. | This last hidden state of encoder is sent as the initial state to the decoder. | During training, we use teacher forcing technique to send inputs to the decoder. | In teacher forcing, the input to the decoder at the next time step will be the actual expected output of the previous time step but not the predicted output. We can observe this in the above figure. At second time step, the predicted output is &#39;says&#39;. Instead of this, we send the actual output &#39;said&#39; as input in the next time step. In this way, the model can learn effectively. | . Below is the code snippet for creating encoder-decoder model with teacher forcing. . This model didn&#39;t predict well. | The main disadvantage with encoder-decoder models is that the context vector(last hidden state of encoder) fails to remember long sentences as it has fixed length. | Hence, we proceeded with attention mechanism. | . ii. Encoder-Decoder Model with Attention Mechanism . Below figure shows Attention Mechanism. This figure is from Lilian Weng&#39;s blog.[4] | . . Image Source Encoder is a bidirectional RNN(can also be unidirectional) and decoder is a unidirectional RNN. | As done in basic encoder decoder model, we send input sentence to the encoder. We get forward and backward encoder hidden states at each timestep. | We get final encoder states(hi) by concatenating the forward and backward states at each timestep. | Input to the decoder at each timestep is the concatenation of expected output of previous time step and context vector. Decoder hidden state will be passed to the next time step. | Unlike basic encoder-decoder models where context vector is calculated only once and of fixed length, in attention, we calculate context vector at every time step. | Below are the equations to calculate context vector. Source For Equations | . Below is the explanation for calculating context vector. . Initial hidden state of decoder, s0 is computed by taking tanh of encoder backward hidden state. | First we assign scores to a pair of decoder and encoder hidden states based on how well they match. Lets consider s0 as the initial hidden state of decoder and h1 as the encoder hidden state at first time step. We assign a score for them using the score equation pointed above where va, Wa are weight matrices. Similarly, we assign scores to s0, h2(encoder hidden state at second time step) and so on. Finally, we get scores for the pairs: (s0,h1), (s0,h2),..,(s0,hn). | Attention weights define how much of each source hidden state should be considered for each output. We calculate attention weights by applying softmax to the scores. Let alpha denotes attention weight. . Lets calculate attention weights for the first time step at decoder.&lt;/br&gt; . alpha 1,1 = softmax(score(s0,h1))&lt;/br&gt; . alpha 1,2 = softmax(score(s0,h2))&lt;/br&gt; . .......................................................&lt;/br&gt; . alpha 1,n = softmax(score(s0,hn))&lt;/br&gt; . | | Now we have all the attention weights for the first time step which define which inputs are most important for the output at this time step. . | Now we can get context vector for the first time step by calculating dot product of above attention weights(alpha 1,1,alpha 1,2,...,alpha 1,n) and all encoder hidden states(h1,h2,...,hn). Context vector is the summation of hidden states of the input sequence weighted by alignment scores. | | In the similar way, we can calculate context vectors at each time step of the decoder. . | As context vector has access to the entire input sequence at each time step, it can memorize long sequences and there are no worries of forgetting. | . Below is the code snippet for creating encoder-decoder model with attention mechanism. . This encoder-decoder model with attention mechanism performed so well in correcting grammatical errors when compared to basic encoder-decoder model. | However, it can&#39;t predict OOV words and in test data we have many OOV words. | Transformers are the best in predicting OOV words as the tranformer tokenizer uses sub-word tokenization or byte-pair encoding to tokenize the words.[10] Hence, proceeded modelling with transformers. | . iii. Transformers . Transformers, especially BERT is one of the latest milestones in handling language based tasks. | Hugging Face provides many libraries with pre-trained Transformer models.[11] | There are different transformer models like BERT, GPT, GPT2, Roberta, XLNet, etc,. | Some transformer models have only encoder part and some have only decoders. | Encoder and Decoders in Transformer models are different from those we have discussed above. | Encoder part is composed of many encoder layers where each layer has self attention layer, feed forword layer. | Decoder part is composed of many decoder layers where each layer has self attention layer, encoder-decoder-attention layer, and feed forward layer. | These encoder and decoder parts are different for different transformer models. There are masked self-attention layers in some transformer models. | Here are great blogs written by Jay Alammar in which he clearly explained every part of the Transformer model.[12] [13] . http://jalammar.github.io/illustrated-transformer/ . http://jalammar.github.io/illustrated-bert/ . | . Tried modelling with GPT2 Transformer and the model is good in correcting grammatical errors and also in predicting OOV words. | However, this model is very large. It has huge parameters. Time complexity is high and also this is difficult to deploy. | Hence, fell back to previous attention mechanism shown in section B and to handle OOV words used transformer based tokenization using pre-tranined tokenizers from hugging face library. | . iv. Attention Mechanism with Transformer based tokenization . Used GPT2 Tokenizer from Hugging Face library to tokenize the data. | GPT2 Tokenizer uses byte-pair encoding for tokenization.[10] | GPT2 Tokenizer calculates Byte-pair encoding as shown below. Count the frequency of each word in the training corpus. Now, we have each word and its corresponding frequency. | Get the base vocabulary with all the characters from the words and then split the words by character. | From all the splitted words, take each pair of symbols(characters) and get the most frequent pair and merge them. | Form merge rules until we get the desired vocabulary size. | As the base vocabulary contains almost all English alphabets, OOV words can be tokenized. | | . Below is the code snippet to tokenize the data using GPT2Tokenizer. . Passed tokenized data as inputs to the attention model. | This model is able to correct the grammatical erros well and also able to predict OOV words. | . Below picture shows the predicted sentences and BLEU scores during inference. . . Final Result on Test Data: . F0.5 score = 0.67 | Perplexity = 35.48485 | . Code for Training and Inference @ https://github.com/VyshnaviVanjari/Deep_Text_Corrector/blob/main/training_inference.ipynb . Error Analysis: . Errors model corrected properly: . Most of the spelling mistakes are properly corrected. | Some errors have wrong verb tense as present tense. Verb past tense is properly predicted. | Some articles were removed and for these errors, articles are correclty predicted. | Some linking words were missing like if, in, at, from, for etc,. Some of these errors are corrected. | Word contractions like &#39;ve, &#39;t, &#39;s, &#39;ll, etc., were removed. Some of these errors are corrected. | Full stop and comma were interchanged and some of these errors are corrected. | Missing quotes are corrected. | . Erros model failed to correct: . Some words are wrongly predicted. | Some errors with combined punctuation and upper case lower case errors are not properly corrected.(eg. , yes --&gt; . Yes) | Some linking words were missing and are not predicted. | &#39;in&#39; is wrongly predicted as &#39;into&#39; sometimes. | Most of the verb forms are predicted properly but few verb forms are not predicted correctly. | Some articles are not predicted properly. | Some word contractions like &#39;d are predicted which are not necessary. | There are some errors with plurals, and some of these are predicted as singulars. | . 6. Future Work . From the above analysis, we can observe that the model is able to correct different kind of errors and at the same time failed to correct same kind of errors. | Almost all spelling mistakes are corrected properly and most of the verb forms are corrected properly. | Some errors are not trained properly as these errors are few. | With much more data and by applying these errors to large data, we can train and predict much more efficiently. | Transformer modelling is a great approach to this problem. Using quantization, we can reduce the model parameters and can try to build the model efficiently. | . 7. References . https://www.pdfdrive.com/harry-potter-the-complete-collection-e187542062.html | https://realpython.com/natural-language-processing-spacy-python/ | https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html | https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#:~:text=The%20attention%20mechanism%20was%20born%20to%20help%20memorize%20long%20source,and%20the%20entire%20source%20input | https://huggingface.co/transformers/tokenizer_summary.html | https://www.aclweb.org/anthology/W14-1701.pdf | https://machinelearningmastery.com/fbeta-measure-for-machine-learning/#:~:text=F%2DMeasure%20%3D%20(2%20*,)%20%2F%20(Precision%20%2B%20Recall) | https://machinelearningmastery.com/calculate-bleu-score-for-text-python/ | https://en.wikipedia.org/wiki/Perplexity | https://huggingface.co/transformers/tokenizer_summary.html | https://huggingface.co/transformers/ | http://jalammar.github.io/illustrated-transformer/ | http://jalammar.github.io/illustrated-bert/ | Thanks for ReadingüòÉ . Complete code in github: https://github.com/VyshnaviVanjari/Deep_Text_Corrector . Reach me at Linkedin: https://www.linkedin.com/in/vyshnavi-vanjari-57345896/ .",
            "url": "https://vyshnavivanjari.github.io/ML-DL-Blogs/deep%20learning/attention/2020/10/31/Deep-Text-Corrector.html",
            "relUrl": "/deep%20learning/attention/2020/10/31/Deep-Text-Corrector.html",
            "date": " ‚Ä¢ Oct 31, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a ‚Äúlevel 1 heading‚Äù in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here‚Äôs a footnote 1. Here‚Äôs a horizontal rule: . . Lists . Here‚Äôs a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes ‚Ä¶and‚Ä¶ . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote.¬†&#8617; . |",
            "url": "https://vyshnavivanjari.github.io/ML-DL-Blogs/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " ‚Ä¢ Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats.¬†&#8617; . |",
          "url": "https://vyshnavivanjari.github.io/ML-DL-Blogs/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://vyshnavivanjari.github.io/ML-DL-Blogs/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}