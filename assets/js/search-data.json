{
  
    
        "post0": {
            "title": "Hard Drive Failure Prediction",
            "content": "Image Source . Introduction to the Business Problem . Hard drives are essential parts of data storage. When a hard disk drive malfunctions and data can‚Äôt be accessed, it is called a Hard Disk Drive Failure. | There are many reasons for the failure of a Hard Drive like high magnetic fields, exposure to heat, or any normal operation, data corruption, human error, power issues etc,. | Failure of a hard drive can be immediate, progressive or limited. Data may be totally destroyed or partially destroyed or can be totally recovered. | So predicting the failures can be helpful in data backup prior to failure and we can replace the drive with a good one without any loss of our data. | A Hard drive failure prediction method called SMART(Self-Monitoring, Analysis and Reporting Technology) has been proposed to constantly monitor the drives to predict failures in order to reduce the risk of data loss. | SMART attributes represent Hard Drive health statistics such as the number of scan errors, reallocation counts and probational counts of a Hard Drive. | In this case study, we are using Backblaze Hard Drive Stats Q3 2019 to predict failures of Hard Drives. | Backblaze provides the data containing information from different manufacturers and different models with all the SMART parameters. | Using the data provided by Backblaze, we apply different machine learning algorithms to predict the failures. | . To pose the problem in a better way, we can say that we need to predict if a hard drive is going to fail in the next &#39;N&#39; days(&#39;N&#39; is optimal value). If we can predict failure before &#39;N&#39; days, we get sufficient time to retrieve the data and can replace that with a new drive. . Data Extraction . The Backblaze data center takes a snapshot of each operational hard drive everyday. The snapshot includes basic drive information along with the SMART statistics reported by the drive. All drives&#39; snapshots for a given day are collected into a file consisting of a row for each active hard drive. The format of this file is &#39;csv&#39;(Comma Separated Values). Each day this file is named in the format YYYY-MM-DD.csv, for example, 2019-08-01.csv. . The columns of the file are as follows: . Date ‚Äì The date of the file in yyyy-mm-dd format. | Serial Number ‚Äì The manufacturer-assigned serial number of the drive. | Model ‚Äì The manufacturer-assigned model number of the drive. | Capacity ‚Äì The drive capacity in bytes. | Failure ‚Äì Contains a &#39;0&#39; if the drive is OK. Contains a &#39;1&#39; if this is the last day the drive was operational before failing. | SMART Parameters | . We can download data from Backblaze website: https://www.backblaze.com/b2/hard-drive-test-data.html#downloading-the-raw-hard-drive-test-data . Existing Approaches . Research Paper: https://hal.archives-ouvertes.fr/hal-01703140/document . In this research paper, machine learning algorithms were applied on the 2013 Backblaze dataset and all the models‚Äô performances were compared. | 12 million samples from 47,793 drives including 31 models from 5 manufacturers were used. Of the 12 million samples, only 2586 samples have failure labels set to 1 and others are healthy samples which makes the dataset highly imbalanced. | Defined a time window for failure which predicts whether the hard drive is going to fail in the next N days. | Some SMART parameters were pre selected based on high correlation to failure events. SMART parameters - 5, 12, 187, 188, 189, 190, 198, 199, 200. | To handle data imbalance, SMOTE technique was applied. | Some failure samples share the exact same feature values as healthy samples leading to the impossibility to discriminate against them and so certain categories of failure samples were filtered. | Logistic Regression, Support Vector Machine, Random Forest Classifier, Gradient Boosting Decision Trees - All these machine learning algorithms were applied on the final data. | It was observed that RF and GBDT resulted in good precision and recall. | RF provided best performances with 95% precision and 67% recall. | . Improvements to the Exisiting Approaches . For every hard drive, SMART parameters are recorded everyday and these parameters are very important in predicting failures. | We can extract time series features from these SMART parameters like rolling window features(mean, std), expanding window features(mean, std), exponential smoothing, lags, etc,. | In the research paper, it was given that SMOTE technique used for data balancing didn&#39;t contributed much in predicting failures. Hence upsampling is used to balance the data. | . Evaluation Metrics . False alarm rate is a good choice for balanced datasets but, as operational datasets are extremely unbalanced in favour of working drives, even a low false alarm rate in the range of 1% could translate into poor performances. Therefore, we report precision, recall metrics and f1-score. . | False Alarm Ratio = False Alarms/Total number of alarms i.e., False Alarm Ratio = (Number of drives wrongly detected as failures)/(Total number of actually failed drives) . | Precision = true positive/(true positive + false positive) i.e., Precision = (Number of drives that are actually failures)/(Total number of drives that are predicted to be failures) . | Recall = true positive/(true positive + false negative) i.e., Recall = (Number of drives predicted correctly as failures)/(Total number of drives that are actually failures) . | F1-Score = 2 (Precision Recall) / (Precision + Recall) . | . Exploratory Data Analysis . Backblaze uses the following five SMART stats as a means to help determine if a drive is going to fail. . ATTRIBUTE DESCRIPTION SMART 5 Reallocated Sectors Count SMART 187 Reported Uncorrectable Errors SMART 188 Command Timeout SMART 197 Current Pending Sector Count SMART 198 Uncorrectable Sector Count . Along with above features, we also selected other SMART features - SMART 5, 9, 12, 187, 188, 189, 190, 193, 194, 197, 198, 199, 200, 241, 242. . df=pd.read_csv(&quot;july_august&quot;+&quot; &quot;+listdir(&quot;july_august&quot;)[0]) for file in listdir(&quot;july_august&quot;)[1:]: df=df.append(pd.read_csv(&quot;july_august&quot;+&quot; &quot;+file)) . Output: . As we can observe, there are normalized features for SMART parameters. But, we only select raw values. Also, due to limited computational power, we select only segate models&#39; July and August months&#39; data. . features=[&#39;date&#39;,&#39;model&#39;,&#39;serial_number&#39;,&#39;capacity_bytes&#39;, &#39;failure&#39;,&#39;smart_5_raw&#39;,&#39;smart_9_raw&#39;,&#39;smart_12_raw&#39;,&#39;smart_187_raw&#39;, &#39;smart_188_raw&#39;,&#39;smart_189_raw&#39;,&#39;smart_190_raw&#39;,&#39;smart_193_raw&#39;,&#39;smart_194_raw&#39;,&#39;smart_197_raw&#39;,&#39;smart_198_raw&#39;, &#39;smart_199_raw&#39;,&#39;smart_200_raw&#39;,&#39;smart_241_raw&#39;,&#39;smart_242_raw&#39;] df_new=df[features] df_segate=df_new[df_new[&#39;model&#39;]==&#39;ST4000DM000&#39;] for model in df_new[&#39;model&#39;].unique()[1:]: if model[:2] == &#39;ST&#39; or model[:2] == &#39;Se&#39;: print(model) row_df=df_new[df_new[&#39;model&#39;]==model] df_segate=pd.concat([df_segate,row_df]) df_segate.reset_index(inplace=True,drop=True) . Univariate Analysis: Failure . sb.set_style(&#39;darkgrid&#39;) sb.distplot(df_segate[&#39;failure&#39;]) plt.title(&quot;Distribution Plot of failure&quot;) plt.show() . . df_segate[df_segate[&#39;failure&#39;]==0].shape (5053997, 20) df_segate[df_segate[&#39;failure&#39;]==1].shape (367, 20) . From the above plot, we can observe that the data set is highly imbalanced with only a few number of failures(failure=1) . Univariate Analysis: smart_5_raw . sb.boxplot(x=&#39;failure&#39;,y=&#39;smart_5_raw&#39;,data=df_segate) plt.title(&quot;Box Plot to detect failure based on smart_5_raw&quot;) plt.show() . . From the above plot, we can observe that most of the drives are working for different range values of smart_5_raw and most of failures occurred when smart_5_raw is below the range of 10000 . sb.distplot(df_segate[df_segate[&#39;failure&#39;]==1][&#39;smart_5_raw&#39;].dropna()) plt.title(&quot;Distribution Plot of smart_5_raw for failed drives&quot;) plt.show() . . From the above plots, we can observe that most of the failures(failure=1) occurred when smart_5_raw values are in the range of 0 to 10000 and few failures in the range of 20000 and one failure in the range of 70000 . Univariate Analysis: smart_9_raw . sb.boxplot(x=&#39;failure&#39;,y=&#39;smart_9_raw&#39;,data=df_segate) plt.title(&quot;Box Plot to detect failure based on smart_9_raw&quot;) plt.show() . . From the above plot, we can observe that working drives have smart_9_raw values in the range of 12000 to 29000. However, some failed drives also have smart_9_raw values in the range of 10000 to 18000 . sb.distplot(df_segate[df_segate[&#39;failure&#39;]==1][&#39;smart_9_raw&#39;].dropna()) plt.title(&quot;Distribution Plot of smart_9_raw for failed drives&quot;) plt.show() . . From the above plots, we can observe that most of the failures occurred when smart_9_raw is in the range of 10000 to 20000 . Univariate Analysis: smart_187_raw . sb.boxplot(x=&#39;failure&#39;,y=&#39;smart_187_raw&#39;,data=df_segate) plt.title(&quot;Box Plot to detect failure based on smart_187_raw&quot;) plt.show() . . From the above plot, we can observe that working drives have smart_187_raw values in different ranges and most of the failures when smart_187_raw value is 0 . sb.distplot(df_segate[df_segate[&#39;failure&#39;]==1][&#39;smart_187_raw&#39;].dropna()) plt.title(&quot;Distribution Plot of smart_187_raw for failed drives&quot;) plt.show() . . From the above plots, we can observe that most of the failures occurred when smart_187_raw values are in the range of 0 to 1000 and only one failure occurred when the values are greater than 50000 . Univariate Analysis: smart_188_raw . sb.boxplot(x=&#39;failure&#39;,y=&#39;smart_188_raw&#39;,data=df_segate) plt.title(&quot;Box Plot to detect failure based on smart_188_raw&quot;) plt.show() . . From the above plot, we can observe that working drives have smart_188_raw values in very high ranges of 1e11 . Univariate Analysis: smart_193_raw . sb.boxplot(x=&#39;failure&#39;,y=&#39;smart_193_raw&#39;,data=df_segate) plt.title(&quot;Box Plot to detect failure based on smart_193_raw&quot;) plt.show() . . From the above plot, we can observe that working drives have smart_193_raw values in different ranges from 0 to 1400000 . sb.distplot(df_segate[df_segate[&#39;failure&#39;]==1][&#39;smart_193_raw&#39;].dropna()) plt.title(&quot;Distribution Plot of smart_193_raw for failed drives&quot;) plt.show() . . From the above plots and analysis, we can observe that most of the failures occurred when smart_193_raw is in the range of 0 to 10000 and few failures in the range of 10000 to 50000 . Univariate Analysis: smart_194_raw . sb.boxplot(x=&#39;failure&#39;,y=&#39;smart_194_raw&#39;,data=df_segate) plt.title(&quot;Box Plot to detect failure based on smart_194_raw&quot;) plt.show() . . From the above plot, we can observe that both working and failed drives have smart_194_raw values in almost same ranges from 25 to 35 . sb.distplot(df_segate[df_segate[&#39;failure&#39;]==1][&#39;smart_194_raw&#39;].dropna()) plt.title(&quot;Distribution Plot of smart_194_raw for failed drives&quot;) plt.show() . . From the above plots, we can observe that most failures occurred when smart_194_raw values are in the range of 20 to 40 . Univariate Analysis: smart_197_raw . sb.boxplot(x=&#39;failure&#39;,y=&#39;smart_197_raw&#39;,data=df_segate) plt.title(&quot;Box Plot to detect failure based on smart_197_raw&quot;) plt.show() . . From the above plot, we can observe that working drives have smart_197_raw values in the range of 0 to 3000 and also failed drives have same range of values . sb.distplot(df_segate[df_segate[&#39;failure&#39;]==1][&#39;smart_197_raw&#39;].dropna()) plt.title(&quot;Distribution Plot of smart_197_raw for failed drives&quot;) plt.show() . . From the above plots, we can observe that all failures occurred when smart_197_raw values are in the range of 0 to 3000 and most failures occurred when the value is 0 . Univariate Analysis: smart_198_raw . sb.boxplot(x=&#39;failure&#39;,y=&#39;smart_198_raw&#39;,data=df_segate) plt.title(&quot;Box Plot to detect failure based on smart_198_raw&quot;) plt.show() . . From the above plot, we can observe that working drives have smart_198_raw values in the range of 0 to 3000 and failed drives also have almost same range of values . sb.distplot(df_segate[df_segate[&#39;failure&#39;]==1][&#39;smart_198_raw&#39;].dropna()) plt.title(&quot;Distribution Plot of smart_198_raw for failed drives&quot;) plt.show() . . From above plot, we can observe that all the failures occurred when smart_198_raw values are in the range of 0 to 2500 and most failures occurred when the value is 0 . Univariate Analysis: smart_241_raw . sb.boxplot(x=&#39;failure&#39;,y=&#39;smart_241_raw&#39;,data=df_segate) plt.title(&quot;Box Plot to detect failure based on smart_241_raw&quot;) plt.show() . . From the above plot, we can observe that working drives have smart_241_raw values in different ranges and have high values in the range of 1e11 . sb.distplot(df_segate[df_segate[&#39;failure&#39;]==1][&#39;smart_241_raw&#39;].dropna()) plt.title(&quot;Distribution Plot of smart_241_raw for failed drives&quot;) plt.show() . . From the above plot, we can observe that failures occurred when smart_241_raw values are in very high ranges of 0.5e11 to 0.6e11 . Univariate Analysis: smart_242_raw . sb.boxplot(x=&#39;failure&#39;,y=&#39;smart_242_raw&#39;,data=df_segate) plt.title(&quot;Box Plot to detect failure based on smart_242_raw&quot;) plt.show() . . From the above plot, we can observe that working drives have smart_242_raw values in different ranges of values from 0 to 2.5e13 . sb.distplot(df_segate[df_segate[&#39;failure&#39;]==1][&#39;smart_242_raw&#39;].dropna()) plt.title(&quot;Distribution Plot of smart_242_raw for failed drives&quot;) plt.show() . . From the above plot and analysis, we can observe that most of the failures occurred when smart_242_raw values are in the range of 1.062508e+10 to 1.5e+11 . Summary of Univariate Analyses: . From all the above plots, we can observe that the failures occurred for different ranges of smart attributes&#39; values. Most failures occurred when smart attributes&#39; values are 0. . Data Preprocessing . Missing Values . We remove features which have missing values&#39; percentage greater than 30. . &#39;&#39;&#39;percentage of mssing values in each column&#39;&#39;&#39; features_new_2=[] for i in df_segate.columns: print(i,null_sum[i]*100/df_segate.shape[0]) if null_sum[i]*100/df_segate.shape[0] &gt; 30: continue features_new_2.append(i) df_segate=df_segate[features_new_2 . More than 25% of the values are missing in smart_189_raw, smart_200_raw. So dropping those columns . Filling Missing Values with mean . We fill missing values in each column with their mean . df_segate[&#39;smart_5_raw&#39;]=df_segate[&#39;smart_5_raw&#39;].fillna(df_segate[&#39;smart_5_raw&#39;].mean()) df_segate[&#39;smart_9_raw&#39;]=df_segate[&#39;smart_9_raw&#39;].fillna(df_segate[&#39;smart_9_raw&#39;].mean()) df_segate[&#39;smart_12_raw&#39;]=df_segate[&#39;smart_12_raw&#39;].fillna(df_segate[&#39;smart_12_raw&#39;].mean()) df_segate[&#39;smart_187_raw&#39;]=df_segate[&#39;smart_187_raw&#39;].fillna(df_segate[&#39;smart_187_raw&#39;].mean()) df_segate[&#39;smart_188_raw&#39;]=df_segate[&#39;smart_188_raw&#39;].fillna(df_segate[&#39;smart_188_raw&#39;].mean()) df_segate[&#39;smart_190_raw&#39;]=df_segate[&#39;smart_190_raw&#39;].fillna(df_segate[&#39;smart_190_raw&#39;].mean()) df_segate[&#39;smart_193_raw&#39;]=df_segate[&#39;smart_193_raw&#39;].fillna(df_segate[&#39;smart_193_raw&#39;].mean()) df_segate[&#39;smart_194_raw&#39;]=df_segate[&#39;smart_194_raw&#39;].fillna(df_segate[&#39;smart_194_raw&#39;].mean()) df_segate[&#39;smart_197_raw&#39;]=df_segate[&#39;smart_197_raw&#39;].fillna(df_segate[&#39;smart_197_raw&#39;].mean()) df_segate[&#39;smart_198_raw&#39;]=df_segate[&#39;smart_198_raw&#39;].fillna(df_segate[&#39;smart_198_raw&#39;].mean()) df_segate[&#39;smart_199_raw&#39;]=df_segate[&#39;smart_199_raw&#39;].fillna(df_segate[&#39;smart_199_raw&#39;].mean()) df_segate[&#39;smart_241_raw&#39;]=df_segate[&#39;smart_241_raw&#39;].fillna(df_segate[&#39;smart_241_raw&#39;].mean()) df_segate[&#39;smart_242_raw&#39;]=df_segate[&#39;smart_242_raw&#39;].fillna(df_segate[&#39;smart_242_raw&#39;].mean()) . Computing mean, std, min, max for smart_parameters row-wise . We calculate Mean, Standard Deviation, Min, Max for SMART parameters row-wise and add them as new features. . df_segate[&#39;mean&#39;]=df_segate[df_segate.columns[5:18]].mean(axis=1) df_segate[&#39;std&#39;]=df_segate[df_segate.columns[5:18]].std(axis=1) df_segate[&#39;min&#39;]=df_segate[df_segate.columns[5:18]].min(axis=1) df_segate[&#39;max&#39;]=df_segate[df_segate.columns[5:18]].max(axis=1) . Removing negative capacity byte values . We have observed that there are some negative values for capacity byte which are wrongly recorded. Hence dropping those rows. . temp=df_segate[df_segate[&#39;capacity_bytes&#39;]&lt;10.0] ind=temp.index df_segate.drop(df_segate.index[list(ind)],inplace=True) . Feature Engineering . Sort the DataFrame by serial_number and date to extract time series features. . df_new=pd.read_csv(&#39;df_segate_july_august_no_backtrack.csv&#39;) df_new_with_lag=df_new.sort_values([&#39;serial_number&#39;,&#39;date&#39;]) . Rolling Mean, Standard Deviation for SMART parameters - window 15 . We can write code using pd.DataFrame.shift function but we shouldn&#39;t do that directly for a column. We have different models with different serial numbers. For every unique serial number, we need to calculate rolling mean and standard deviation. We can write code with shift function by looping over all the unique serial numbers but it takes a lot of time as there are many thousands of unique serial numbers. Hence written code as pointed below. Here we take window=15. . serial_numbers=df_new_with_lag[&#39;serial_number&#39;].values serial_number=df_new_with_lag[&#39;serial_number&#39;].values[0] for column in tqdm(df_new_with_lag.columns[5:18]): rolling_mean=[] rolling_stdev=[] for i in range(df_new_with_lag.shape[0]): if serial_numbers[i]!=serial_numbers[i-1]: values=[] values.append(df_new_with_lag[column].values[i]) rolling_mean.append(mean(values)) rolling_stdev.append(values[-1]) else: if(len(values)&lt;15): values.append(df_new_with_lag[column].values[i]) mean_=mean(values[0:len(values)]) stdev_=stdev(values[0:len(values)]) rolling_mean.append(mean_) rolling_stdev.append(stdev_) else: values.append(df_new_with_lag[column].values[i]) mean_=mean(values[len(values)-15:len(values)]) stdev_=stdev(values[len(values)-15:len(values)]) rolling_mean.append(mean_) rolling_stdev.append(stdev_) df_new_with_lag[column+&#39;_rolling_mean&#39;] = rolling_mean df_new_with_lag[column+&#39;_rolling_stdev&#39;] = rolling_stdev . Expanding Mean for SMART parameters . Below is the code snippet for extracting expanding mean features. . serial_numbers=df_new_with_lag[&#39;serial_number&#39;].values serial_number=df_new_with_lag[&#39;serial_number&#39;].values[0] for column in tqdm(df_new_with_lag.columns[5:18]): expanding_mean=[] expanding_stdev=[] for i in range(df_new_with_lag.shape[0]): if serial_numbers[i]!=serial_numbers[i-1]: values=[] values.append(df_new_with_lag[column].values[i]) expanding_mean.append(sum(values)) expanding_stdev.append(values[-1]) else: values.append(df_new_with_lag[column].values[i]) expanding_mean.append(mean(values)) expanding_stdev.append(stdev(values)) df_new_with_lag[column+&#39;_expanding_mean&#39;] = expanding_mean df_new_with_lag[column+&#39;_expanding_stdev&#39;] = expanding_stdev . Exponential Smoothing . For exponential smoothing, we have considered alpha=0.15. . serial_numbers=df_new_with_lag[&#39;serial_number&#39;].values serial_number=df_new_with_lag[&#39;serial_number&#39;].values[0] alpha=0.15 for column in tqdm(df_new_with_lag.columns[5:18]): predicted_values=[] for i in range(df_new_with_lag.shape[0]): if serial_numbers[i]!=serial_numbers[i-1]: predicted_value = (df_new_with_lag[column].values)[i] predicted_values.append(predicted_value) else: predicted_value =(alpha*df_new_with_lag[column].values[i]) + ((1-alpha)*predicted_value) predicted_values.append(predicted_value) df_new_with_lag[column+&#39;_exp_avg&#39;] = predicted_values df_new_with_lag=df_new_with_lag.sort_values([&#39;date&#39;]) . Change Failure status by backtracking . We need to predict whether a drive is going to fail in the next &#39;N&#39; days. Here we take N=15. For this, we backtrack last 15 days&#39; failures, i.e., if we have a failed drive, we mark failure as &#39;1&#39; for its previous 15 days. . df_segate_backtrack=pd.read_csv(&quot;df_segate.csv&quot;) new_date=[] for date in df_segate_backtrack[&#39;date&#39;]: new_date.append(datetime.strptime(date,&#39;%Y-%m-%d&#39;).date()) df_segate_backtrack[&#39;date&#39;]=new_date failed=df_segate_backtrack[df_segate_backtrack[&#39;failure&#39;]==1] for serial_number in tqdm(failed.serial_number): d=failed[failed[&#39;serial_number&#39;]==serial_number].date.values temp=df_segate_backtrack[df_segate_backtrack[&#39;serial_number&#39;]==serial_number] temp=temp[temp.date&gt;=(d[0]-timedelta(days=15))] temp=temp[temp.date&lt;d[0]] indices=temp.index df_segate_backtrack.loc[indices,&#39;failure&#39;]=1 failed=df_segate_backtrack[df_segate_backtrack[&#39;failure&#39;]==1] . Train-Test split, Response Coding Categorical Features and Upsampling Minority Class . After extracting the time series features and backtracking the failure status, we have splitted the data into train, cv, and test. We have extracted features from Model ID and Serial Number and encoded them using response coding. Below are the features extracted from Model ID and Serial Number: . model_char_count - eg: Model ID = &#39;ST12000NM0007&#39;, model_char_count = 13 | model_second_last_char - eg: Model ID = &#39;ST12000NM0007&#39;, model_second_last_char = &#39;T7&#39; | serial_number_second_last_char - eg: Serial Number = &#39;ZJV2SP8Y&#39;, serial_number_second_last_char = &#39;JY&#39; | . By response coding model_second_last_char and serial_number_second_last_char, we get working and failing probabilites for these features out of which we used working probabilites as features. . Finally, to balance the data, we upsampled the minority class(drives with failure=1). . failed_train_upsample_df=resample(failed_train_df,replace=True, n_samples=int(working_train_df.shape[0]), random_state=42) . Modelling . Data is standardized and used for Logistic Regression and Support Vector Machines. For Naive Bayes, data is normalized. Data is used directly for Random Forests and XGBoost as these are based on decision trees. For all the below models, small code snippets and results are printed. . Logistic Regression . Logistic Regression is one of the useful algorithms for Binary Classification. The assumption that is made for this algorithm is that the data is linear.[1][2] . Hyper-parameter tuning is performed for different c_range values and L2 Regularization is used. . c_range=[10**-4,10**-3,10**-2,10**-1,10**0,10**1,10**2,10**3,10**4] for c in c_range: LR_model=LogisticRegression(C=c,penalty=&#39;l2&#39;) LR_model.fit(train_standardized,train_output) . Observed that f1 score is very less with logistic regression. For c=0.01 and with penalty=&#39;l2&#39;, we got the best test f1 score. . train_f1_score=0.751285 | cv_f1_score=0.009825 | test_f1_score=0.009673 | . Support Vector Machine . Support Vector Machine (SVM) alogirthm relies on finding the hyperplane that splits the two classes to predict while maximizing the distance with the closest data points.[1][2] . SGDClassifier with loss=&#39;Hinge&#39; is used and hyper-parameter tuning is performed for different ranges of alpha. . alpha_range=[10**-4,10**-3,10**-2,10**-1,10**0,10**1,10**2,10**3,10**4] for a in alpha_range: SGD_model=SGDClassifier(loss=&#39;hinge&#39;,alpha=a) SGD_model.fit(train_standardized,train_output) . Observed that test f1 score is less with SVM. For alpha=0.0001, we got the best test f1 score. . train_f1_score=0.756617 | cv_f1_score=0.011170 | test_f1_score=0.01123 | . Random Forest Classifier . Random forest is an ensemble technique based on Decision Tress. It takes a subset of observations and a subset of variables to build a group of decision trees. Predictions are made based on a vote among the different decision trees. Random forest model is chosen as it is robust to noise, caused by poorly correlated SMART features.[1][2] . Hyper-parameter tuning is performed for different values of n_estimators, max_depth, learning_rate. Probabilities are calibrated using CalibratedClassiferCV before predicting the target values. . n_estimators = [100,150,200] max_depth = [7, 9] for i in n_estimators: for j in max_depth: rf_model = RandomForestClassifier(n_estimators=i, max_depth=j) rf_model.fit(train_df_final_1,train_output) cal_rf_model=CalibratedClassifierCV(rf_model,method=&#39;isotonic&#39;,cv=&#39;prefit&#39;) cal_rf_model.fit(cv_df_final_1,cv_output) . Observed that test f1 score is less with RandomForestClassifier. For n_estiamtors=150, max_depth=9, we got the best test f1 score. . train_f1_score=0.27872 | cv_f1_score=0.19057 | test_f1_score=0.02651 | . XGBClassifier . Gradient boosted tree (GBT) is another ensemble technique based on decision trees. Training takes place in an iterative fashion with the goal of trying to minimize a loss function using a gradient descent method.[1][2] . Hyper-parameter tuning is performed for different values of n_estimators and max_depth. Probabilities are calibrated using CalibratedClassiferCV before predicting the target values. . n_estimators = [100, 150, 200, 500, 1000] max_depth = [7, 9] for i in n_estimators: for j in max_depth: xgb_model = xgb.XGBClassifier(n_estimators=i, max_depth=j,tree_method=&#39;exact&#39;) xgb_model.fit(train_df_final_1,train_output) cal_xgb_model=CalibratedClassifierCV(xgb_model,method=&#39;isotonic&#39;,cv=&#39;prefit&#39;) cal_xgb_model.fit(cv_df_final_1,cv_output) important_features=xgb_model.get_booster().get_score(importance_type=&quot;gain&quot;) important_features_sorted=sorted(important_features.items(),key=lambda x:x[1], reverse=True) index_features=dict() i=0 for column in train_df_final.columns: index_features[&#39;f&#39;+str(i)]=column i+=1 sorted_important_features_dict=dict() for i in important_features_sorted: key=i[0] sorted_important_features_dict[key]=index_features[key] print(&quot;Top 10 important features:&quot;) list(sorted_important_features_dict.items())[0:10] . . Observed that XGBoost performed very well in predicting failed hard drives. Optimal hyper-parameters: n_estimators=1000, max_depth=9 . test f1_score: 0.929026 | test Precison : 0.943334 | test Recall : 0.915139 | . Naive Bayes . The Naive Bayes classifier makes the assumption that the value of a feature is conditionally independent of the value of another feature given some class label. Among the different techniques used for building Naive Bayes models, we chose Multinomial Naive Bayes, which assumes that the probability of a feature value given some class label is sampled from a multinomial distribution. For regularization, we use Laplace smoothing.[2] . Hyper-parameter tuing is performed for different ranges of alpha. . alpha_range=[10**-4,10**-3,10**-2,10**-1,10**0,10**1,10**2,10**3,10**4] for a in alpha_range: nb_clf=MultinomialNB(alpha=a, fit_prior=True) nb_clf.fit(train_normalized,train_output) . Observed that test f1 score is less with Naive Bayes. For alpha=0.0001, we got the best test f1 score. . train_f1_score=0.625518 | cv_f1_score=0.004463 | test_f1_score=0.004925 | . XGBClassifier with top 50 important features . n_estimators = [100, 150, 200, 500, 1000] max_depth = [7, 9] for i in n_estimators: for j in max_depth: xgb_model_imp = xgb.XGBClassifier(n_estimators=i, max_depth=j,tree_method=&#39;exact&#39;) xgb_model_imp.fit(train_df_final_imp_1,train_output) cal_xgb_model_imp=CalibratedClassifierCV(xgb_model_imp,method=&#39;isotonic&#39;,cv=&#39;prefit&#39;) cal_xgb_model_imp.fit(cv_df_final_imp_1,cv_output) . Observed that f1 score is good with XGBClassifier when compared with all other models. With n_estimators=1000, max_depth=9, we got the best f1 score. . test f1_score : 0.935266 | test Precison : 0.9370764762826719 | test Recall : 0.9334619093539055 | . Ensemble of RandomForestClassifier and XGBClassifier with top 50 important features . Tried ensembling with different combinations of above classifiers with different weights using EnsembleVoteClassifier. Ensemble of RandomForestClassifer and XGBClassifier with more weight to XGBClassifier resulted in good prediction. . ensemble_vote_clf_imp = EnsembleVoteClassifier(clfs=[cal_rf_model_imp,cal_xgb_model_imp_new], voting=&#39;soft&#39;,refit=False,weights=[0.1,0.9]) ensemble_vote_clf_imp.fit(train_df_final_imp_1,train_output) predicted_test_failure=ensemble_vote_clf_imp.predict(test_df_final_imp_1) test_f1_scores.append(f1_score(test_output, predicted_test_failure)) print(&quot;test Precison :&quot;,precision_score(test_output,predicted_test_failure)) print(&quot;test Recall :&quot;,recall_score(test_output,predicted_test_failure)) print(&quot;test_f1_score=&quot;,test_f1_scores[-1]) . test Precison : 0.9477317554240631 | test Recall : 0.926711668273867 | test_f1_score= 0.9371038517796197 | . Comparision of Different Models . . By comparing results of above two tables, we can observe that with top 50 important features we are able to get good scores. We got best f1 score with XGBClassifier. The next good model is RandomForest but it doesn&#39;t perform that well. . . Summary: . From the above tables, we can observe that precision is good with random forests and xgboost but is very less with other classifiers. | Recall is good with all classifiers except random forests. | XGBClassifier predicted failed hard drives very well. | Precision and recall scores are highest with XGBClassifier and also with ensemble of RF and XGB. . | XGB With top 50 important features: . Test Precison : 0.937076 Test Recall : 0.933461 -- Best Test f1_score: 0.935266 . | Ensemble of XGB and RF With top 50 important features: . Test Precison : 0.947731 -- Best Test Recall : 0.926711 Test f1_score: 0.937103 -- Best . | We can observe that recall score is high with XGB classifier when top 50 features are used. Whereas f1-score and precision are high with ensemble of XGB and RF with top 50 features. We can choose any of these two models. Both the models are good. . | Extracted many time series features from given data like exponential averages, rolling mean, rolling standard deviation, expanding mean, expanding standard deviation, backtracked last 15 days&#39; failures etc,. | Top 10 important features for XGBClassifier are: smart_188_raw_exp_avg smart_5_raw smart_197_raw model_second_last_char_working capacity_bytes smart_199_raw_expanding_stdev serial_second_last_char_working smart_188_raw_expanding_mean smart_9_raw_rolling_mean smart_12_raw_rolling_stdev | The above results are on SEGATE model hard drives&#39; July and August months data. We can try with XGBoost modelling for other hard drives also. | Recall is the important metric here. Our main aim to detect failed hard drives. In this case study, we have predicted hard drives that are going to fail in the next 15 days. If we can predict the drives that are going to fail few days before the failure, we can have sufficient time to retrieve data and replace them with new hard drives. It is somewhat fine if a drive predicted to be a failure is actually a working one. But the important aim here is recall: drives which are actually failures should be predicted as failures else if wrongly predicted as working ones, it may fail in future and data can&#39;t be retrieved. | We got best recall score with XGBClassifier with top 50 important features: 0.933461 | Limited data to arorund 5 million due to limited system capacity. Train data is around 3 millions(after upsampling around 6 million) With more amounts of data and feature engineering, we can further improve recall and f1 scores. | Future Work . From the above summary, we can observe that many of the time series features are useful in predicting hard drive failures. | So, we can extract and experiment with more time series features for much better results. Eg: double/triple exponential smoothing, lag features, etc,. | We can use more data from all quarters of an Year to improve the results. | We can also experiment with deep learning techniques to predict the failures. | References . https://hal.archives-ouvertes.fr/hal-01703140/document | http://cs229.stanford.edu/proj2017/final-reports/5242080.pdf | https://www.kaggle.com/vishakg/predicting-hdd-failures-using-ml | https://en.wikipedia.org/wiki/Hard_disk_drive_failure | https://www.backblaze.com/blog/backblaze-hard-drive-stats-q3-2019/ | https://neurospace.io/blog/2018/10/predicting-hard-drive-failure-with-machine-learning/ | https://vsbytes.com/hdd-vs-ssd/ | https://www.securedatarecovery.com/services/hard-drive-recovery/how-to-determine-a-hard-drive-failed-physically | Thanks for ReadingüòÉ . Complete code in github: https://github.com/VyshnaviVanjari/HDDFailure . Reach me at Linkedin: https://www.linkedin.com/in/vyshnavi-vanjari-57345896/ .",
            "url": "https://vyshnavivanjari.github.io/ML-DL-Blogs/machine%20learning/2020/10/31/Hard-Drive-Failure-Prediction.html",
            "relUrl": "/machine%20learning/2020/10/31/Hard-Drive-Failure-Prediction.html",
            "date": " ‚Ä¢ Oct 31, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Deep Text Corrector",
            "content": "Introduction . This project aims to correct grammatical mistakes in a given English text. | Grammatical errors are of different types, including errors in articles or determiners, prepositions, noun form, verb form, subject-verb agreement, pronouns, word choice, sentence structure, punctuation, capitalization, etc. | In this project, we aim to correct 16 types of grammatical errors. . Verb Tense | Subject-Verb agreement | Missing Verb | Article or Determiner | Noun Number | Noun Possessive | Pronoun Reference | Run-on Sentences, Comma Splices | Adjective/Adverb order | Linking words/Phrases | Poor Citation | Parallelism | Improper Preposition Forms | Word Contractions | Capitalization | Spelling | | . To achieve this, we train a Deep Learning Model which takes grammatically incorrect text as input and outputs grammatically corrected text. | Sequence-to-Sequence models are capable of correcting grammatical errors. | We try different models with different architectures starting from basic encoder-decoder models, encoder-decoder models with attention to Transformers. | . Data Preparation . Downloaded an English Novel which has grammatically correct text.[1] | Did some basic pre-processing. Removed contents, chapters&#39; names, etc, to just have only paragraphs with text. | Did sentence detection using SPACY by setting custom boundaries.[2] | . Below is a small code snippet for setting custom boundaries. . #sentence detection def set_custom_boundaries(doc): for i,token in enumerate(doc[:-1]): if token.text == &#39;)&#39; and doc[i-1].text == &#39;.&#39; and i&gt;0: doc[i+1].is_sent_start = True elif token.text == &#39;;&#39; and not doc[i+1].text.islower(): doc[i+1] .is_sent_start = True elif token.text == &#39;!&#39; and not doc[i+1].text.islower(): doc[i+1] .is_sent_start = True elif token.text == &#39;?&#39; and not doc[i+1].text.islower(): doc[i+1] .is_sent_start = True elif token.text == &#39;‚Äù&#39; and (doc[i-1].text==&#39;.&#39; or doc[i-1].text==&#39;?&#39; or doc[i-1].text==&#39;!&#39; or doc[i-1].text==&#39;;&#39;) and i&gt;0 and not doc[i+1].text.islower(): doc[i+1].is_sent_start = True elif token.text == &#39;.&#39; and doc[i+1].text==&#39;)&#39;: doc[i+1].is_sent_start = False elif token.text == &#39;.&#39; and doc[i+1].text!=&#39;.&#39; and not doc[i+1].text.islower(): doc[i+1].is_sent_start = True else: doc[i+1].is_sent_start = False return doc . . Some of the sentences generated from above process have improper opening and closing quotes, braces. Corrected them to get linguistically meaningful units. | Removed sentences having only one word. | Once we got all the grammatically correct meaningful sentences, splitted the data into train, cv and test data. | Introduced 16 different types of grammatical errors into the datasets. | . Below is the code snippet for introducing 16 types of errors into the sentences. . &quot;&quot;&quot; words after splitting string and tokens in spacy nlp(string) are different as spacy treats punctuations included in a word as seperate tokens Eg: &quot;&#39;my name don is the &#39;was the man then&quot;.split() output: [&quot;&#39;my&quot;, &#39;name&#39;, &#39;don&#39;, &#39;is&#39;, &#39;the&#39;, &quot;&#39;was&quot;, &#39;the&#39;, &#39;man&#39;, &#39;then&#39;] for t in nlp(&quot;&#39;my name don is the &#39;was the man then&quot;): print(t) output: &#39; my name don is the &#39; was the man then If we observe, apostrophe(&#39;) is considered as seperate token in spacy. Here we are considering punctuations and defining errors. For example, replacing comma with fullstop, removing opening double quote, etc. But, we can&#39;t replace tokens in spacy. So, we convert it into string first and find index of the token to be replaced. Below function finds index, and replaces tokens with error. &quot;&quot;&quot; #function to replace tokens with errors def replace_with_error(about_inc,index,to_replace,error): l=str(about_inc) g=[] for i,t in enumerate(about_inc): if(i==index) and to_replace==about_inc[index].text: idx=t.idx elif(i==index): idx=t.idx-to_replace.find(about_inc[index].text) lent=len(to_replace) skip=0 for i,ch in enumerate(l): if skip: skip-=1 continue if(i==idx): g.append(error) skip=lent-1 else: g.append(ch) return &#39;&#39;.join(g) #function to define errors def get_incorrect_lines(df): incorrect_lines=[] words_end_with=(&#39;ch&#39;,&#39;ss&#39;,&#39;sh&#39;,&#39;x&#39;,&#39;zz&#39;) words_ends_with_cont=(&quot;&#39;ve&quot;,&quot;&#39;ll&quot;,&quot;&#39;s&quot;,&quot;&#39;m&quot;,&quot;&#39;d&quot;,&quot;&#39;re&quot;,&quot;&#39;t&quot;) vowels=(&#39;ay&#39;,&#39;ey&#39;,&#39;iy&#39;,&#39;oy&#39;,&#39;uy&#39;) nlp = spacy.load(&#39;en_core_web_sm&#39;) for i in tqdm(range(len(df.values))): about_doc = nlp(df.values[i]) about_inc=nlp(df.values[i]) #counts for not repeating same error twice in a line count1=0;count2=0;count3=0;count4=0;count5=0;count6=0;count7=0;count8=0 count9=0;count10=0;count11=0;count12=0;count13=0;count14=0;count15=0;count16=0 #count for maintaining only two errors per line error_count=0 skip=0 previous_tokens=[] for j,token in enumerate(about_doc): #storing tokens previous_tokens.append(token.text) #getting index for tokens in incorrect lines(about_inc) for t in about_inc: if about_doc[j].text==t.text: index=t.i break if skip: skip -= 1 continue #if same token is present twice in the line, skipping that token if about_doc[j].text in previous_tokens[0:-1]: skip=1 if skip: skip -= 1 continue #1. replacing Verb Past Tense(VBD) to Verb present tense: were --&gt; are, saw --&gt; see if token.tag_ == &#39;VBD&#39; and about_inc[index].text!=token.lemma_ and about_inc[index].text[0].islower() and count1==0 and error_count&lt;=2: about_inc=nlp(replace_with_error(about_inc,index,about_inc[index].text,token.lemma_)) count1=1 #2. Subject-verb agreement: they go --&gt;they goes,we watch --&gt;we watches elif token.tag_ == &#39;VBP&#39;: for t in nlp(str(token)): if t.tag_ == &#39;VB&#39;: if about_inc[index].text.endswith(words_end_with) or about_inc[index].text == &#39;go&#39; or about_inc[index].text == &#39;do&#39; and count2==0 and error_count&lt;=2: about_inc=nlp(replace_with_error(about_inc,index,about_inc[index].text,about_inc[index].text+&#39;es&#39;)) count2=1 elif about_inc[index].text.endswith(vowels) and count2==0: about_inc=nlp(replace_with_error(about_inc,index,about_inc[index].text,about_inc[index].text+&#39;s&#39;)) count2=1 elif about_inc[index].text.endswith(&#39;y&#39;) and count2==0: about_inc=nlp(replace_with_error(about_inc,index,about_inc[index].text,about_inc[index].text.replace(&#39;y&#39;,&#39;ies&#39;))) count2=1 elif about_inc[index].text == &#39;be&#39; and count2==0: about_inc=nlp(replace_with_error(about_inc,index,about_inc[index].text,&#39;is&#39;)) count2=1 elif about_inc[index].text.endswith(&#39;s&#39;) and count2==0: about_inc=nlp(replace_with_error(about_inc,index,about_inc[index].text,about_inc[index].text+&#39;ses&#39;)) count2=1 elif about_inc[index].text.endswith(&#39;z&#39;) and count2==0: about_inc=nlp(replace_with_error(about_inc,index,about_inc[index].text,about_inc[index].text+&#39;zes&#39;)) count2=1 #3. Missing verb: However, there are also a great number of people [who ‚Üí who are] against if about_inc[index-1].tag_ == &#39;WP&#39; and j!=0 and count3==0: about_inc=nlp(replace_with_error(about_inc,index,about_inc[index].text+&quot; &quot;,&quot;&quot;)) count3=1 #4. Removing Articles elif about_inc[index].text==&#39;a&#39; or about_inc[index].text==&#39;an&#39; or about_inc[index].text==&#39;the&#39; and count4==0: about_inc=nlp(replace_with_error(about_inc,index,about_inc[index].text+&quot; &quot;,&quot;&quot;)) count4=1 #5. Noun number: children --&gt; child elif token.tag_ == &#39;NNS&#39; and about_inc[index].text!=token.lemma_ and about_inc[index].text[0].islower() and count5==0: about_inc=nlp(replace_with_error(about_inc,index,about_inc[index].text,token.lemma_)) count5=1 #6. Noun possessive: carrier&#39;s --&gt; carriers elif about_inc[index].text==&quot;&#39;s&quot; and count6==0: about_inc=nlp(replace_with_error(about_inc,index,about_inc[index].text,&#39;s&#39;)) count6=1 #7. Pronoun reference: they --&gt; he/she elif about_inc[index].text == &#39;they&#39; and count7==0: about_inc=nlp(replace_with_error(about_inc,index,about_inc[index].text,&#39;he&#39;)) count7=1 #8. Run-on sentences,comma splices: The issue is highly [debatable, a ‚Üí debatable. A] genetic risk elif about_inc[index].text.islower() == False and about_inc[index].text!=&#39;‚Äú&#39; and str(about_inc[index-1]) == &#39;.&#39; and not str(about_inc).startswith(about_inc[index].text) and not str(about_inc).endswith(&#39;.&#39;) and about_inc[index].text!=&#39;I&#39; and count8==0: about_inc=nlp(replace_with_error(about_inc,index,about_inc[index-1].text+&quot; &quot;+about_inc[index].text,&#39;, &#39;+about_inc[index].text.lower())) count8=1 #9. Incorrect adjective/adverb order: In conclusion, [personally I ‚Üí I personally] feel that it elif token.tag_ == &#39;RB&#39; and count9==0 and not &quot;&#39;&quot; in about_inc[index].text and len(about_inc[index].text)&gt;=2 and not str(about_inc).startswith(about_inc[index].text): punc=0 for p in string.punctuation: if p not in about_inc[index].text or p not in about_inc[index-1].text: punc=1 break if(punc==0): about_inc=nlp(replace_with_error(about_inc,index,about_inc[index-1].text+&quot; &quot;+about_inc[index].text,about_inc[index].text+&quot; &quot;+about_inc[index-1].text)) count9=1 #10. Linking words/phrases: It is sometimes hard to find [out ‚Üí out if] one has this disease. elif token.tag_ == &#39;IN&#39; and j!=0 and count10==0: about_inc=nlp(replace_with_error(about_inc,index,about_inc[index].text+&quot; &quot;,&quot;&quot;)) count10=1 #11. Poor citation elif (about_inc[index].text == &#39;‚Äú&#39;) and count11==0: temp=str(about_inc) for word in temp.split(): if word.startswith(&#39;‚Äú&#39;): word1=word.replace(&#39;‚Äú&#39;,&#39;‚Äú &#39;) temp=temp.replace(word,word1) elif word.endswith(&#39;‚Äù&#39;): word1=word.replace(&#39;‚Äù&#39;,&#39; ‚Äù&#39;) temp=temp.replace(word,word1) if &#39;‚Äú&#39; in temp.split() and j!=temp.split().index(&#39;‚Äú&#39;) and not temp.endswith(&#39;‚Äù&#39;): about_inc=nlp(replace_with_error(about_inc,index,about_inc[index].text,&quot;&quot;)) count11=1 #13. Parallelism: We must pay attention to this information and [assisting ‚Üí assist] those elif about_inc[index].text.endswith(&#39;ing&#39;) and about_inc[index-1].tag_ == &#39;CC&#39; and about_inc[index].text!=token.lemma_ and about_inc[index].text[0].islower() and count13==0: about_inc=nlp(replace_with_error(about_inc,index,about_inc[index].text,token.lemma_)) count13=1 #14. Improper preposition forms: This essay will [discuss about ‚Üí discuss] whether a carrier elif &#39;discuss&#39; in about_inc[index].text and count14==0: about_inc=nlp(replace_with_error(about_inc,index,about_inc[index].text,about_inc[index].text+&quot; &quot;+&#39;about&#39;)) count14=1 elif about_inc[index].text==&#39;through&#39; and count14==0: about_inc=nlp(replace_with_error(about_inc,index,about_inc[index].text,&#39;to&#39;)) count14=1 elif about_inc[index].text==&#39;into&#39; and count14==0: about_inc=nlp(replace_with_error(about_inc,index,about_inc[index].text,&#39;in&#39;)) count14=1 elif about_inc[index].text==&#39;since&#39; and count14==0: about_inc=nlp(replace_with_error(about_inc,index,about_inc[index].text,&#39;for&#39;)) count14=1 #15. Removing word contractions: I&#39;ve--&gt;I, we&#39;ll--&gt;we, let&#39;s--&gt;let elif about_inc[index].text.endswith(words_ends_with_cont) and count15==0: if about_inc[index].text == &quot;n&#39;t&quot;: about_inc=nlp(replace_with_error(about_inc,index,&quot;&#39;t&quot;,&quot;&quot;)) else: about_inc=nlp(replace_with_error(about_inc,index,about_inc[index].text,&quot;&quot;)) count15=1 #16. upper to lower, lower to upper: elif len(str(about_inc).split())&lt;=2 and count16==0: if about_inc[index].text.isupper(): about_inc=nlp(replace_with_error(about_inc,index,about_inc[index].text,about_inc[index].text.lower())) count16=1 else: about_inc=nlp(replace_with_error(about_inc,index,about_inc[index].text,about_inc[index].text.upper())) count16=1 #12. Incorrect Spelling elif len(about_inc[index].text)&gt;2 and about_inc[index].tag_ != &#39;WP&#39; and about_inc[index].tag_ != &#39;NNP&#39; and &quot;&#39;&quot; not in about_inc[index].text and &quot;&#39;&quot; not in about_inc[index+1].text and count12==0: if &quot;&#39;&quot; not in about_inc[index-1].text or str(about_inc).startswith(about_inc[index].text): #swapping letters for wrong spelling c1=about_inc[index].text[1] c2=about_inc[index].text[2] c0=about_inc[index].text[0] if c1!=c2: d=about_inc[index].text.replace(c2,&quot;-&quot;,1) d=d.replace(c1,c2,1) d=d.replace(d[2],c1,1) else: d=about_inc[index].text.replace(c1,c0,1) about_inc=nlp(replace_with_error(about_inc,index,about_inc[index].text,d)) count12=1 #skipping tokens to avoid errors in three adjacent tokens if index&gt;=len(about_inc): skip=2 elif about_inc[index].text != about_doc[j].text: skip=2 incorrect_lines.append(str(about_inc)) return incorrect_lines . . After introducing errors into the sentences, introuduced space between words and punctuations in the sentences as punctuations need to be considered as seperate tokens while tokenization. Eg: &#39;I am good.&#39; - &#39;I am good .&#39; (space between good and fullstop) | With this, we get proper train, cv, test datas with grammatically incorrect input sentences and grammatically correct output sentences. | . Complete Code for Pre-Processing and Introducting errors @ https://github.com/VyshnaviVanjari/Deep_Text_Corrector/blob/main/preprocessing_add_errors.ipynb . Approaches to the Problem . Encoder-Decoder models using RNNs is one of the approaches to this problem. However, this has a disadvatange of having a fixed length context vector which is incapable of remembering long sentences. [3] | The attention mechanism helps to memorize long source sentences in neural machine translation. Rather than building a single context vector from the encoder&#39;s last hidden state, attention creates shortcuts between the context vector and the entire source input.[4] | Attention mechanism is a very good approach to this problem but the only problem is with Out of Vocabulary(OOV) words. | Transformers are a great solution to this. Transformer tokenizers provide sub-word tokenization, byte-pair encoding, etc., and so there will be no problem in tokenizing OOV words.[5] | . Evaluation Metrices . Chosen F0.5 as the evaluation metric similar to that in CoNLL-2014 shared task on grammatical error correction. F0.5 emphasizes precision twice as much as recall, while F1 weighs precision and recall equally. For a grammar checker, it is important that its corrections are highly accurate. Neglecting correction of an error is not as bad as providing erroneous correction. [6] [7] . Precision = True Positives/(True Positives + False Positives) . Recall = True Positives/(True Positives + False Negatives) . F1-Score = (2 Precision Recall) / (Precision + Recall) . Fbeta = ((1 + beta^2) Precision Recall) / (beta^2 * Precision + Recall) . F0.5-score (beta=0.5): More weight on precision, less weight on recall. . | . During inference, reported BLEU scores, perplexity along with F0.5 score.[8] [9] | . Modelling . Encoder Decoder Model with Teacher Forcing . Below figure shows an encoder decoder model with teacher forcing. . Input: Harry say nothing. Expected Output: Harry said nothing. . | . . Data needs to be tokenized and embedded before sending to encoder and decoder. | As we can see in the above figure, we send the input sentence(embedded one) to the encoder. Here encoder and decoders are RNNs like LSTMs. | Encoder encodes the sentence and the last hidden state has all the sentence information. | This last hidden state of encoder is sent as the initial state to the decoder. | During training, we use teacher forcing technique to send inputs to the decoder. | In teacher forcing, the input to the decoder at the next time step will be the actual expected output of the previous time step but not the predicted output. We can observe this in the above figure. At second time step, the predicted output is &#39;says&#39;. Instead of this, we send the actual output &#39;said&#39; as input in the next time step. In this way, the model can learn effectively. | . Below is the code snippet for creating encoder-decoder model with teacher forcing. . encoder_inputs = Input(shape=(None,), name=&quot;enc_input_layer&quot;) embed = Embedding(input_dim=vocab_size, output_dim=300, weights=[embedding_matrix], trainable=False, name=&quot;enc_dec_embedding_layer&quot;) enc_embed=embed(encoder_inputs) &#39;&#39;&#39;return states and discard output as we will use only the encoder states at the last timestep which is the context vector as input for the decoder&#39;&#39;&#39; encoder_out,state_h,state_c = LSTM(300,return_state=True,name=&quot;enc_lstm_layer&quot;)(enc_embed) encoder_states=[state_h,state_c] decoder_inputs = Input(shape=(None,),name=&#39;dec_input_layer&#39;) dec_embed=embed(decoder_inputs) decoder_out,state_h,state_c=LSTM(300,return_state=True,return_sequences=True)(dec_embed,initial_state=encoder_states) dec_dense=Dense(vocab_size, activation=&#39;softmax&#39;)(decoder_out) encoder_decoder_model = Model(inputs=[encoder_inputs,decoder_inputs], outputs=dec_dense, name=&quot;encoder_decoder_model&quot;) . . This model didn&#39;t predict well. | The main disadvantage with encoder-decoder models is that the context vector(last hidden state of encoder) fails to remember long sentences as it has fixed length. | Hence, we proceeded with attention mechanism. | . Encoder-Decoder Model with Attention Mechanism . Below figure shows Attention Mechanism. This figure is from Lilian Weng&#39;s blog.[4] | . . Image Source Encoder is a bidirectional RNN(can also be unidirectional) and decoder is a unidirectional RNN. | As done in basic encoder decoder model, we send input sentence to the encoder. We get forward and backward encoder hidden states at each timestep. | We get final encoder states(hi) by concatenating the forward and backward states at each timestep. | Input to the decoder at each timestep is the concatenation of expected output of previous time step and context vector. Decoder hidden state will be passed to the next time step. | Unlike basic encoder-decoder models where context vector is calculated only once and of fixed length, in attention, we calculate context vector at every time step. | Below are the equations to calculate context vector. . . Source For Equations | . Below is the explanation for calculating context vector. . Initial hidden state of decoder, s0 is computed by taking tanh of encoder backward hidden state. | First we assign scores to a pair of decoder and encoder hidden states based on how well they match. Lets consider s0 as the initial hidden state of decoder and h1 as the encoder hidden state at first time step. We assign a score for them using the score equation pointed above where va, Wa are weight matrices. Similarly, we assign scores to s0, h2(encoder hidden state at second time step) and so on. Finally, we get scores for the pairs: (s0,h1), (s0,h2),..,(s0,hn). | Attention weights define how much of each source hidden state should be considered for each output. We calculate attention weights by applying softmax to the scores. Let alpha denotes attention weight. Lets calculate attention weights for the first time step at decoder. . alpha 1,1 = softmax(score(s0,h1)) . alpha 1,2 = softmax(score(s0,h2)) . ....................................................... . alpha 1,n = softmax(score(s0,hn)) . | Now we have all the attention weights for the first time step which define which inputs are most important for the output at this time step. | Now we can get context vector for the first time step by calculating dot product of above attention weights(alpha 1,1,alpha 1,2,...,alpha 1,n) and all encoder hidden states(h1,h2,...,hn). Context vector is the summation of hidden states of the input sequence weighted by alignment scores. | | In the similar way, we can calculate context vectors at each time step of the decoder. . | As context vector has access to the entire input sequence at each time step, it can memorize long sequences and there are no worries of forgetting. | . Below is the code snippet for creating encoder-decoder model with attention mechanism. . #https://arxiv.org/pdf/1409.0473.pdf #https://www.tensorflow.org/tutorials/text/nmt_with_attention #https://udai.gitbook.io/practical-ml/nn/training-and-debugging-of-nn #fixing numpy RS np.random.seed(42) #fixing tensorflow RS tf.random.set_seed(32) #python RS rn.seed(12) hidden_units=512 #encoder inputs encoder_inputs = Input(shape=(max_length,),dtype=tf.int64,name=&quot;enc_input_layer&quot;) #encoder_mask encoder_mask = Input(shape=(max_length,),dtype=tf.int64,name=&quot;encoder_mask&quot;) #decoder inputs decoder_input = Input(shape=(max_length,),dtype=tf.int64,name=&#39;dec_input_layer&#39;) #decoder_mask decoder_mask = Input(shape=(max_length,),dtype=tf.int64,name=&quot;decoder_mask&quot;) #Embedding layer embed_layer = Embedding(len(tokenizer),output_dim=512,name=&#39;enc_dec_embedding_layer&#39;) encoder_GRU_layer_1=GRU(hidden_units,dropout=0.1,return_sequences=True,name=&#39;encoder_GRU_layer_1&#39;) encoder_GRU_layer_2=GRU(hidden_units,dropout=0.1,return_sequences=True,name=&#39;encoder_GRU_layer_2&#39;) encoder_GRU_layer_3=GRU(hidden_units,dropout=0.1,return_sequences=True,return_state=True,name=&#39;encoder_GRU_layer_3&#39;) enc_embed=embed_layer(encoder_inputs) encoder_out=encoder_GRU_layer_1(enc_embed) encoder_out=encoder_GRU_layer_2(encoder_out) encoder_out,encoder_hidden=encoder_GRU_layer_3(encoder_out) weights_dense_layer1=Dense(hidden_units,name=&#39;attn_score_weights_layer1&#39;) weights_dense_layer2=Dense(hidden_units,name=&#39;attn_score_weights_layer2&#39;) score_dense_layer=Dense(1,name=&#39;attn_score_dense_layer&#39;) decoder_GRU_layer_1=GRU(hidden_units,dropout=0.1,return_sequences=True,return_state=True,name=&#39;decoder_GRU_layer_1&#39;) decoder_GRU_layer_2=GRU(hidden_units,dropout=0.1,return_sequences=True,name=&#39;decoder_GRU_layer_2&#39;) layer_normalization=LayerNormalization(name=&#39;layer_norm_layer&#39;) #final dense layer decoder_dense = Dense(len(tokenizer),activation=&#39;softmax&#39;,name=&#39;dec_dense_layer&#39;) decoder_hidden=tf.nn.tanh(encoder_hidden) all_outputs=[] for i in tqdm(range(max_length)): #teacher forcing - giving actual output of previous time step as input - initial input is &#39;START&#39; dec_inp=tf.gather(decoder_input,[i],axis=1) dec_mask=tf.gather(decoder_mask,[i],axis=1) #we are doing this to broadcast addition along the time axis to calculate the score decoder_hidden_with_time_axis=tf.expand_dims(decoder_hidden,1) #getting context_vector from attention layer score=score_dense_layer(tf.nn.tanh(weights_dense_layer1(decoder_hidden_with_time_axis)+weights_dense_layer2(encoder_out))) score=tf.squeeze(score,[-1]) scores_mask=tf.cast(encoder_mask,dtype=tf.bool) padding_mask = tf.logical_not(scores_mask) score-=1.e9 * tf.cast(padding_mask, dtype=score.dtype) attention_weights=tf.nn.softmax(score,axis=1) attention_weights = tf.expand_dims(attention_weights, 1) context_vector=tf.matmul(attention_weights,encoder_out) context_vector=tf.squeeze(context_vector,1) context_vector*=tf.cast(tf.cast(dec_mask,dtype=tf.bool),dtype=context_vector.dtype) dec_inp=embed_layer(dec_inp) dec_inp=tf.concat([tf.expand_dims(context_vector,1),dec_inp],axis=-1) decoder_out,decoder_hidden=decoder_GRU_layer_1(dec_inp,initial_state=decoder_hidden) decoder_out=layer_normalization(decoder_out) decoder_out=decoder_GRU_layer_2(decoder_out) decoder_out=layer_normalization(decoder_out) decoder_out=decoder_GRU_layer_2(decoder_out) decoder_out=layer_normalization(decoder_out) out=decoder_dense(decoder_out) all_outputs.append(out) decoder_outputs=Lambda(lambda x: concatenate(x,axis=1))(all_outputs) attn_model = Model(inputs=[encoder_inputs,encoder_mask,decoder_input,decoder_mask],outputs=decoder_outputs,name=&#39;attn_model&#39;) . . This encoder-decoder model with attention mechanism performed so well in correcting grammatical errors when compared to basic encoder-decoder model. | However, it can&#39;t predict OOV words and in test data we have many OOV words. | Transformers are the best in predicting OOV words as the tranformer tokenizer uses sub-word tokenization or byte-pair encoding to tokenize the words.[10] Hence, proceeded modelling with transformers. | . Transformers . Transformers, especially BERT is one of the latest milestones in handling language based tasks. | Hugging Face provides many libraries with pre-trained Transformer models.[11] | There are different transformer models like BERT, GPT, GPT2, Roberta, XLNet, etc,. | Some transformer models have only encoder part and some have only decoders. | Encoder and Decoders in Transformer models are different from those we have discussed above. | Encoder part is composed of many encoder layers where each layer has self attention layer, feed forword layer. | Decoder part is composed of many decoder layers where each layer has self attention layer, encoder-decoder-attention layer, and feed forward layer. | These encoder and decoder parts are different for different transformer models. There are masked self-attention layers in some transformer models. | Here are great blogs written by Jay Alammar in which he clearly explained every part of the Transformer model.[12] [13] . http://jalammar.github.io/illustrated-transformer/ . http://jalammar.github.io/illustrated-bert/ . | . Tried modelling with GPT2 Transformer and the model is good in correcting grammatical errors and also in predicting OOV words. | However, this model is very large. It has huge parameters. Time complexity is high and also this is difficult to deploy. | Hence, fell back to previous attention mechanism discussed above and to handle OOV words used transformer based tokenization using pre-tranined tokenizers from hugging face library. | . Attention Mechanism with Transformer based tokenization . Used GPT2 Tokenizer from Hugging Face library to tokenize the data. | GPT2 Tokenizer uses byte-pair encoding for tokenization.[10] | GPT2 Tokenizer calculates Byte-pair encoding as shown below. Count the frequency of each word in the training corpus. Now, we have each word and its corresponding frequency. | Get the base vocabulary with all the characters from the words and then split the words by character. | From all the splitted words, take each pair of symbols(characters) and get the most frequent pair and merge them. | Form merge rules until we get the desired vocabulary size. | As the base vocabulary contains almost all English alphabets, OOV words can be tokenized. | | . Below is the code snippet to tokenize the data using GPT2Tokenizer. . tokenizer = GPT2TokenizerFast.from_pretrained(&quot;gpt2&quot;) #cls_token is the start token of the sentence and eos_token is the end token of the sentence tokenizer.add_special_tokens({&quot;cls_token&quot; : &quot;&lt;cls&gt;&quot;,&quot;eos_token&quot; : &quot;&lt;eos&gt;&quot;,&quot;pad_token&quot;: &quot;&lt;pad&gt;&quot;}) train_input_ids, train_input_masks = [],[] for sentence in tqdm(train[&#39;incorrect_lines&#39;]): train_inputs = tokenizer.encode_plus(tokenizer.cls_token+sentence, add_special_tokens=True, max_length=max_length,pad_to_max_length=True, return_attention_mask=True, truncation=True) train_input_ids.append(train_inputs[&#39;input_ids&#39;]) train_input_masks.append(train_inputs[&#39;attention_mask&#39;]) . . Passed tokenized data as inputs to the attention model. | This model is able to correct the grammatical erros well and also able to predict OOV words. | . Below picture shows the predicted sentences and BLEU scores during inference. . . Final Result on Test Data: . F0.5 score = 0.67 | Perplexity = 35.48485 | . Code for Training and Inference @ https://github.com/VyshnaviVanjari/Deep_Text_Corrector/blob/main/training_inference.ipynb . Error Analysis: . Errors model corrected properly: . Most of the spelling mistakes are properly corrected. | Some errors have wrong verb tense as present tense. Verb past tense is properly predicted. | Some articles were removed and for these errors, articles are correclty predicted. | Some linking words were missing like if, in, at, from, for etc,. Some of these errors are corrected. | Word contractions like &#39;ve, &#39;t, &#39;s, &#39;ll, etc., were removed. Some of these errors are corrected. | Full stop and comma were interchanged and some of these errors are corrected. | Missing quotes are corrected. | . Errors model failed to correct: . Some words are wrongly predicted. | Some errors with combined punctuation and upper case lower case errors are not properly corrected.(eg. , yes --&gt; . Yes) | Some linking words were missing and are not predicted. | &#39;in&#39; is wrongly predicted as &#39;into&#39; sometimes. | Most of the verb forms are predicted properly but few verb forms are not predicted correctly. | Some articles are not predicted properly. | Some word contractions like &#39;d are predicted which are not necessary. | There are some errors with plurals, and some of these are predicted as singulars. | . Future Work . From the above analysis, we can observe that the model is able to correct different kind of errors and at the same time failed to correct same kind of errors. | Almost all spelling mistakes are corrected properly and most of the verb forms are corrected properly. | Some errors are not trained properly as these errors are few. | With much more data and by applying these errors to large data, we can train and predict much more efficiently. | Transformer modelling is a great approach to this problem. Using quantization, we can reduce the model parameters and can try to build the model efficiently. | . References . https://www.pdfdrive.com/harry-potter-the-complete-collection-e187542062.html | https://realpython.com/natural-language-processing-spacy-python/ | https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html | https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#:~:text=The%20attention%20mechanism%20was%20born%20to%20help%20memorize%20long%20source,and%20the%20entire%20source%20input | https://huggingface.co/transformers/tokenizer_summary.html | https://www.aclweb.org/anthology/W14-1701.pdf | https://machinelearningmastery.com/fbeta-measure-for-machine-learning/#:~:text=F%2DMeasure%20%3D%20(2%20*,)%20%2F%20(Precision%20%2B%20Recall) | https://machinelearningmastery.com/calculate-bleu-score-for-text-python/ | https://en.wikipedia.org/wiki/Perplexity | https://huggingface.co/transformers/tokenizer_summary.html | https://huggingface.co/transformers/ | http://jalammar.github.io/illustrated-transformer/ | http://jalammar.github.io/illustrated-bert/ | https://blog.cengage.com/six-best-grammar-websites-for-your-college-students/ | Thanks for ReadingüòÉ . Complete code in github: https://github.com/VyshnaviVanjari/Deep_Text_Corrector . Reach me at Linkedin: https://www.linkedin.com/in/vyshnavi-vanjari-57345896/ .",
            "url": "https://vyshnavivanjari.github.io/ML-DL-Blogs/deep%20learning/attention/2020/10/31/Deep-Text-Corrector.html",
            "relUrl": "/deep%20learning/attention/2020/10/31/Deep-Text-Corrector.html",
            "date": " ‚Ä¢ Oct 31, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am a Machine Learning Student at AAIC. Had more than one and half years of experience on Secure Boot Testing at Qualcomm. Check my profile @ Linkedin and Github .",
          "url": "https://vyshnavivanjari.github.io/ML-DL-Blogs/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://vyshnavivanjari.github.io/ML-DL-Blogs/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}